\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Reproducible Research Workflows with Snakemake and R},
            pdfauthor={Lachlan Deer; Julian Langer},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Reproducible Research Workflows with Snakemake and \texttt{R}}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \subtitle{An Extended Tutorial for Economists and Social Scientists}
  \author{Lachlan Deer \\ Julian Langer}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-02-05}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{chapter}{Prerequisites}

This is a \emph{sample} book written in \textbf{Markdown}. You can use
anything that Pandoc's Markdown supports, e.g., a math equation
\(a^2 + b^2 = c^2\).

The \textbf{bookdown} package can be installed from CRAN or Github:

\chapter{Motivating \& Rationale}\label{intro}

\section{A Case for Reproducibility}\label{a-case-for-reproducibility}

\subsection{How far to go in the quest for
reproducibility?}\label{how-far-to-go-in-the-quest-for-reproducibility}

\section{\texorpdfstring{What is \texttt{Snakemake} \& Why Should you
use
it?}{What is Snakemake \& Why Should you use it?}}\label{what-is-snakemake-why-should-you-use-it}

\section{\texorpdfstring{Why \texttt{R}?}{Why R?}}\label{why-r}

\section{Working Example: Replicating Mankiw, Romer and Weil's 1992
QJE}\label{working-example-replicating-mankiw-romer-and-weils-1992-qje}

Throughout our tutorial we are going to use a running example to
illustrate the concepts we discuss.

\section{The way forward}\label{the-way-forward}

For the purpose of this tutorial we will focus on replicating the
following aspects of the MRW paper:\footnote{A complete replication
  using the concepts presented in this tutorial is available
  \textbf{here}}

\begin{itemize}
\tightlist
\item
  Regression Tables 1 and 2: Estimating the Textbook- and Augmented
  Solow Model
\item
  Figure 1: Unconditional Versus Conditional Convergence
\end{itemize}

To replicate these we will need to proceed as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform some data management

  \begin{itemize}
  \tightlist
  \item
    Prepare the data before we run regressions
  \end{itemize}
\item
  Do some analysis. For example, run regressions for:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Different subsets of data
  \item
    Alternative econometric specifications
  \end{enumerate}
\item
  Turn the statistical output of the regressions into a tabular format
  that we can insert into a document
\item
  Construct a set of graphs
\item
  Integrate the tables and graphs into a paper and a set of slides
  (optional)
\end{enumerate}

We hope that these 5 steps look familiar - as they were designed to
represent a simplifed workflow for an applied economist or social
science researcher.

Before proceeding to understanding how to use Snakemake and R to
construct a reproducible workflow, the next chapter first takes a deeper
dive into the a protypical way to set up a research project on our
computer.

\subsection*{Exercise: Your own project's
steps}\label{exercise-your-own-projects-steps}
\addcontentsline{toc}{subsection}{Exercise: Your own project's steps}

Think about a project you are working on or have worked on in the past
(it may be a Bachelor or Master's thesis or a recent / active research
project). Does your project fit into the 5 steps we described above? If
not, what would you modify or add to our 5 steps? (Do you think this
would destroy the general principles we will encourage over the next
chapters?)

\chapter*{PART I}\label{part-i}
\addcontentsline{toc}{chapter}{PART I}

\chapter{Project Organization}\label{project-organization}

\section{Project Structure I: Separating Inputs and
Outputs}\label{project-structure-i-separating-inputs-and-outputs}

Structuring our project and the locations of files is an important
concept.

Let's look at the structure of our project's folder. Open a terminal and
change into this directory

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cd} \NormalTok{YOUR/PATH/TO/snakemake-econ-r-student}
\end{Highlighting}
\end{Shaded}

And list the subdirectories of the main directory

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls} \NormalTok{-d */}
\end{Highlighting}
\end{Shaded}

We see the following folder structure

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{./}
    \KeywordTok{|-} \NormalTok{src/}
    \KeywordTok{|-} \NormalTok{out/}
    \KeywordTok{|-} \NormalTok{log/}
    \KeywordTok{|-} \NormalTok{sandbox/}
\end{Highlighting}
\end{Shaded}

We recommend the following structure for any project:

\begin{itemize}
\tightlist
\item
  Root Folder
\item
  \texttt{src} folder for input files
\item
  \texttt{out} folder for output files
\item
  a \texttt{log} folder to store computer logs
\item
  a \texttt{sandbox} folder that gives us a `safe place' to develop new
  code
\end{itemize}

We discuss each of these in turn.

\subsection{The Root Folder}\label{the-root-folder}

TBD

\subsection{\texorpdfstring{The \texttt{src}
folder}{The src folder}}\label{the-src-folder}

TBD

\subsection{\texorpdfstring{The \texttt{out}
folder}{The out folder}}\label{the-out-folder}

TBD

\subsection{\texorpdfstring{The \texttt{log}
folder}{The log folder}}\label{the-log-folder}

\subsection{Exploring the Full Structure of the MRW Replication
Project}\label{exploring-the-full-structure-of-the-mrw-replication-project}

Now, let's look at all contents of this main projects directory:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls} \NormalTok{-F .}
\end{Highlighting}
\end{Shaded}

We see the following folder structure

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{./}
    \KeywordTok{|-} \NormalTok{src/}
    \KeywordTok{|-} \NormalTok{out/}
    \KeywordTok{|-} \NormalTok{log/}
    \KeywordTok{|-} \NormalTok{sandbox/}
    \KeywordTok{|} \KeywordTok{README.md}
    \KeywordTok{|} \KeywordTok{Snakefile}
\end{Highlighting}
\end{Shaded}

Notice that there are no instances of: (i) scripts, (ii) files
containing content of the paper or slides (iii) something else we
haven't thought of yet Instead, there are only two files, a
\texttt{README.md} and a file called \texttt{Snakefile.}

TODO: explain these two files

\section{Project Structure II: Separating Logical Chunks of the
Project}\label{project-structure-ii-separating-logical-chunks-of-the-project}

As we have mentioned above, to keep our project's structure clean, we
want to keep all the computer code inside the \texttt{src} directory.
Let's have a look at the content of \texttt{src}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls} \NormalTok{-F src/}
\end{Highlighting}
\end{Shaded}

We see the following output:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{./}
    \KeywordTok{|src/}
        \KeywordTok{|-} \NormalTok{data/}
        \KeywordTok{|-} \NormalTok{data-management/}
        \KeywordTok{|-} \NormalTok{data-specs/}
        \KeywordTok{|-} \NormalTok{analysis/}
        \KeywordTok{|-} \NormalTok{model-specs/}
        \KeywordTok{|-} \NormalTok{lib/}
        \KeywordTok{|-} \NormalTok{figures/}
        \KeywordTok{|-} \NormalTok{tables/}
\end{Highlighting}
\end{Shaded}

The type of content we expect in each file is:

TBD

\subsection{Exploring the Structure of the MRW Replication
Subdirectories}\label{exploring-the-structure-of-the-mrw-replication-subdirectories}

We begin our exploration of the project by looking at the folders that
appear to be related to the data. If we look inside the \texttt{data}
directory

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls} \NormalTok{-F src/data/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mrw.dta}
\end{Highlighting}
\end{Shaded}

That is, our \texttt{data/} directory contains the project's original
data set.

Note that in more extensive projects, the \texttt{data/} subfolder would
typically have more than one data set. For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dataset1.dta}
\KeywordTok{dataset2.dta}
\KeywordTok{dataset3.csv}
\end{Highlighting}
\end{Shaded}

TBD - aside on file endings.

Further, your data folder may even contain further subdirectories that
organize data further

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{./}
    \KeywordTok{|src/}
        \KeywordTok{|-} \NormalTok{data/}
            \KeywordTok{|-} \NormalTok{data-provider-a/}
                \KeywordTok{|-} \NormalTok{dataset1.csv}
                \KeywordTok{|-} \NormalTok{dataset2.csv}
            \KeywordTok{|-} \NormalTok{data-provider-b/}
                \KeywordTok{|-} \NormalTok{dataset3.txt}
                \KeywordTok{|-} \NormalTok{dataset4.txt}
\end{Highlighting}
\end{Shaded}

If we now turn to the \texttt{data-management} directory, we can explore
it's contents too:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls} \NormalTok{-F src/data-management/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rename_variables.R}
\KeywordTok{gen_reg_vars.R}
\end{Highlighting}
\end{Shaded}

TODO:

\begin{itemize}
\tightlist
\item
  meaningful filenames
\item
  Note two different ways to name files
\end{itemize}

\subsection*{Exercise: Exploring the Remaining
Subdirectories}\label{exercise-exploring-the-remaining-subdirectories}
\addcontentsline{toc}{subsection}{Exercise: Exploring the Remaining
Subdirectories}

TBD

\section{Project Structure III: Separating Input Parameters from
Code}\label{project-structure-iii-separating-input-parameters-from-code}

Next we look at the somewhat mysteriously named \texttt{data-specs}
folder.

And if we explore the folder's contents:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls} \NormalTok{-F src/data-specs/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{subset_intermediate.json}
\KeywordTok{subset_nonoil.json}
\KeywordTok{subset_oecd.json}
\end{Highlighting}
\end{Shaded}

Again, the file names are somewhat meaningful on their own - they appear
to be some way of subsetting data (selecting some rows). If we look
inside one of these files:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat} \NormalTok{src/data-specs/subset_oecd.json}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{\{}
    \StringTok{"KEEP_CONDITION"}\NormalTok{: }\StringTok{"oecd == 1"}
\KeywordTok{\}}
\end{Highlighting}
\end{Shaded}

We see an a variable \texttt{KEEP\_CONDITION} which is storing a string
\texttt{"oecd\ ==\ 1"}.

TBD: Why have we done this? See below.

\subsection{Exploring Parameter Separation in the MRW Replication
Project}\label{exploring-parameter-separation-in-the-mrw-replication-project}

\chapter{Initial Steps with
Snakemake}\label{initial-steps-with-snakemake}

\section{Starting a Research Project}\label{starting-a-research-project}

We are now ready to get started working with the code and data to build
a fully reproducible pipeline. In Chapter XX we described a simplified
research workflow to be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform some data management
\item
  Do some analysis
\item
  Turn the output of the analysis into a tabular format
\item
  Construct a set of graphs
\item
  Integrate the tables and graphs into a paper and a set of slides
  (optional)
\end{enumerate}

We are going to start at the beginning with data management.

Recall that we have the following files in our data management
subdirectory, \texttt{src/data-management}:

\begin{verbatim}
rename_variables.R
gen_reg_vars.R
\end{verbatim}

We will need to run each of these scripts sequentially. First we want to
run the script \texttt{rename\_variables.R} to tidy up the variable
names in our data set. Second, \texttt{gen\_reg\_vars.R} will create the
some additional variables in our data that will be needed to run some
regressions in later steps. Over the next few sections we are going to
build up 2 \textbf{rules}, one for each file, that will execute these
scripts and deliver output.

\section{The Beginning of a
Snakefile}\label{the-beginning-of-a-snakefile}

We are going to put the collection of rules that build our project into
a file. We can then use the \texttt{Snakemake} to execute these rules
and build our project. The set of rules we want to construct are going
to go into the file called \texttt{Snakefile} - which is the name of a
file that Snakemake will look into by default to execite a project. Lets
open the file called \texttt{Snakefile} in the project's main directory.
When you open it it should look as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Main Workflow - SOME PROJECT}
\CommentTok{#}
\CommentTok{# Contributors: YOUR NAME(S)}
\end{Highlighting}
\end{Shaded}

Note that the amount of structure we have here is not totally necessary.
However, good structure will make understanding easier later. Let's go
through what we see. The first lines of code are comments, to help us
navigate a little and understand what we are looking at. The very first
line tells us that this is a project workflow, and then tells us what
the particular project is. The second line tells us who contributed to
this file. This can be useful so we know who to contact with questions.
You should do update the name of the project, and add your name to the
list of contributors. For us, the top 2 lines becomes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Main Workflow - MRW Replication}
\CommentTok{#}
\CommentTok{# Contributors: @lachlandeer, @julianlanger}
\end{Highlighting}
\end{Shaded}

The next few lines are:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# --- Main Build Rules --- #}
\CommentTok{## To be constructed}
\end{Highlighting}
\end{Shaded}

These are more comments. We are using the
\texttt{\#\ -\/-\/-\ Something\ -\/-\/-\ \#} notation to break up the
code into logical blocks. It is in this block that we will assemble the
rules on which our project will be built.

\section{Rule Structure}\label{rule-structure}

A \texttt{Snakefile} is a collection of rules that together define the
order in which a project will be executed. In our \texttt{Snakefile} we
will start to assemble rules under the
\texttt{\#\ -\/-\/-\ Main\ Build\ Rules\ -\/-\/-\ \#} section to keep
things tidy. Each rule can be thought of as a recipe that combines
different \textbf{inputs}, such as data and and R script together to
produce one or more \textbf{output(s)}. The key components we are going
to use to construct a rule are:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  a name for the rule,
\item
  the list of inputs
\item
  the list of outputs produced
\item
  a shell command that tells snakemake how to combine the inputs to
  produce a outputs.
\end{enumerate}

\texttt{Snakemake} expects these components to be provided in a
particular way so that it knows what to do with the information you
provided. We are going to specify rules in the following format:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{rule_name:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{input_name1} \NormalTok{= }\StringTok{"PATH/TO/input_one"}\NormalTok{,}
        \KeywordTok{input_name2} \NormalTok{= }\StringTok{"PATH/TO/input_two"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{output_name1} \NormalTok{= }\StringTok{"PATH/TO/SAVE/output_one"}\NormalTok{,}
        \KeywordTok{output_name2} \NormalTok{= }\StringTok{"PATH/TO/SAVE/output_two"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"HOW TO MIX IT ALL TOGETHER"}
\end{Highlighting}
\end{Shaded}

We can have as many inputs and outputs as we need to have per rule. Each
input and each output are given names, for example \texttt{input\_name1}
which take the value to the file path and name of the file. It is
important to wrap each of these paths into quotations, and to separate
each of the multiple inputs and outputs with a comma.

\section{Our First Rule}\label{our-first-rule}

\subsection{Constructing the Rule}\label{constructing-the-rule}

As mentioned above, we will start with the data management step.

First script to run is \texttt{rename\_variables.R} which is located in
the data management subdirectory. This is a simple script that renames
some variables for us to be easier to understand.

We can then start our snakemake script by adding this script as an
input:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{rule_name:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/data-management/rename_variables.R"}\NormalTok{,}
        \KeywordTok{input_name2} \NormalTok{= }\StringTok{"PATH/TO/input_two"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{output_name1} \NormalTok{= }\StringTok{"PATH/TO/SAVE/output_one"}\NormalTok{,}
        \KeywordTok{output_name2} \NormalTok{= }\StringTok{"PATH/TO/SAVE/output_two"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"HOW TO MIX IT ALL TOGETHER"}
\end{Highlighting}
\end{Shaded}

Next, we want to add any additional inputs and also specify any outputs
that the file produces. We have set up all R scripts in this example to
provide us with `help' so that we know what we might need to add. To
find out what inputs are required and what outputs are produced, we use
the \texttt{-\/-help} flag when calling the file with \texttt{R}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{Rscript} \NormalTok{src/data-management/rename_variables.R --help}
\end{Highlighting}
\end{Shaded}

And the following output is produced:

\begin{verbatim}
Usage: src/data-management/rename_variables.R [options]


Options:
    -d CHARACTER, --data=CHARACTER
        stata dataset file name

    -o CHARACTER, --out=CHARACTER
        output file name [default = out.csv]

    -h, --help
        Show this help message and exit
\end{verbatim}

This suggests the script needs \ldots{}

We update our \texttt{rename\_variables} as:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{rename_vars:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/data-management/rename_variables.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"src/data/mrw.dta"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{data} \NormalTok{= }\StringTok{"out/data/mrw_renamed.csv"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"HOW TO MIX IT ALL TOGETHER"}
\end{Highlighting}
\end{Shaded}

Next we provide a recipe telling snakemake how to mix the inputs to
create the outputs:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{rename_vars:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/data-management/rename_variables.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"src/data/mrw.dta"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{data} \NormalTok{= }\StringTok{"out/data/mrw_renamed.csv"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --out \{output.data\}"}
\end{Highlighting}
\end{Shaded}

We can now try and run snakemake to execute this rule \ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake}
\end{Highlighting}
\end{Shaded}

Stuff happens \ldots{}

\subsection{Analysing Output}\label{analysing-output}

We can look into our output directory to see if anything has happened:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{ls} \NormalTok{out/data/}
\end{Highlighting}
\end{Shaded}

which yields

\begin{verbatim}
mrw_renamed.csv
\end{verbatim}

Our file has been created as we expected.

Try and run snakemake again

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake}
\end{Highlighting}
\end{Shaded}

and we see the following output:

\begin{verbatim}
Building DAG of jobs...
Nothing to be done.
\end{verbatim}

Why?

Snakemake provides the a \emph{summary} option which tells us what is
going on:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{snakemake} \NormalTok{--summary}
\end{Highlighting}
\end{Shaded}

The output is:

\begin{verbatim}
Building DAG of jobs...
output_file                       date                    rule      version  log-file(s)    status  plan
out/data/mrw_renamed.csv    Thu Jan 10 20:31:08 2019    rename_vars    -                     ok     no update
\end{verbatim}

Explain what this tells us \ldots{}

Suppose we updated one of the inputs \ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{touch} \NormalTok{src/data-management/rename_variables.R}
\end{Highlighting}
\end{Shaded}

and then look at the summary from snakemake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Building DAG of jobs...
output_file                        date                    rule     version log-file(s)      status                plan
out/data/mrw_renamed.csv    Thu Jan 10 20:31:08 2019    rename_vars    -                updated input files update pending
\end{verbatim}

What this means?

Run snakemake to build the output:

\begin{verbatim}
snakemake
\end{verbatim}

\subsection*{Exercise: Deleting Output}\label{exercise-deleting-output}
\addcontentsline{toc}{subsection}{Exercise: Deleting Output}

Delete the output \texttt{out/data/mrw\_renamed.csv}. Run
\texttt{snakemake\ -\/-summary} and explain the output it produced.

\section{Creating a Second Rule}\label{creating-a-second-rule}

The second step in our data management is to create some variables we
will use in our regression analysis. The script
\texttt{gen\_reg\_vars.R} in the \texttt{src/data-management} folder
does this for us. We are going to build a rule called
\texttt{gen\_regression\_vars} to do this in Snakemake. Let's see what
the script expects to be passed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{Rscript} \NormalTok{src/data-management/gen_reg_vars.R --help}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Usage: src/data-management/gen_reg_vars.R [options]


Options:
    -d CHARACTER, --data=CHARACTER
        a csv file name

    -p CHARACTER, --param=CHARACTER
        a file name containing model parameters

    -o CHARACTER, --out=CHARACTER
        output file name [default = out.csv]

    -h, --help
        Show this help message and exit
\end{verbatim}

So we need to provide:

to provide the output \ldots{}

Let's create this rule:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{gen_regression_vars:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/data-management/gen_reg_vars.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_renamed.csv"}\NormalTok{,}
        \KeywordTok{params} \NormalTok{= }\StringTok{"src/data-specs/param_solow.json"}\NormalTok{,}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{data} \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --param \{input.params\} \textbackslash{}}
\StringTok{            --out \{output.data\}"}
\end{Highlighting}
\end{Shaded}

What will Snakemake want to do next? Let's use the summary option to see
\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{--summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Building DAG of jobs...
output_file                    date                     rule                  version   log-file(s) status     plan
out/data/mrw_complete.csv   -                           gen_regression_vars     -                   missing update pending
out/data/mrw_renamed.csv    Fri Jan 11 13:40:07 2019    rename_vars             -                   ok          no update
\end{verbatim}

Explain what this means

Let's run snakemake to build our new file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake}
\end{Highlighting}
\end{Shaded}

When we look at our output directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{ls} \NormalTok{out/data/}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mrw_complete.csv  mrw_renamed.csv
\end{verbatim}

So we see that our data set has been built.

\subsection*{Exercise: Creating Rules}\label{exercise-creating-rules}
\addcontentsline{toc}{subsection}{Exercise: Creating Rules}

The MRW paper estimates the Solow model for three subsets of data. You
need to create rules to do estimate the Solow model for each of these
data sets. The R script \texttt{src/analysis/estimate\_ols.R} will
estimate a OLS model for a given dataset when you provide the necessary
inputs.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What inputs do you need to provide?
\item
  What outputs will be produced?
\item
  Write Snakemake rules to estimate the solow model for each subset of
  data.
\end{enumerate}

\section{Clean Rules}\label{clean-rules}

So far, we have built up our Snakefile to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clean data
\item
  Run a regression model on different subsets of data
\end{enumerate}

As we continue to extend our Snakefile in the coming chapters we might
want to be able to delete all of the produced outputs, and see if we can
rebuild our project from the first step. One way to do this would be to
go to our terminal window and enter the following command each time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{rm} \NormalTok{-rf out/*}
\end{Highlighting}
\end{Shaded}

Instead of doing this each time, we can integrate this \emph{cleaning}
of computer produced outputs into our Snakefile. We can create a rule
called \texttt{clean} that stores the shell command from above:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{clean:}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"rm -rf out/*"}
\end{Highlighting}
\end{Shaded}

Note that this rule has no inputs or outputs.

To use this rule, we enter the following into our terminal:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{clean}
\end{Highlighting}
\end{Shaded}

Notice that to use the clean rule we had to call the rule name, clean,
explicitly.

Now if we look out the output of running the summary call with
snakemake, we see the following output:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{snakemake} \NormalTok{--summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Building DAG of jobs...
output_file                                        date   rule  version log-file(s)     status  plan
out/analysis/model_solow_subset_intermediate.rds    -     inter        -                missing update pending
out/analysis/model_solow_subset_nonoil.rds          -     nonoil       -                missing update pending
out/analysis/model_solow_subset_oecd.rds            -     oecd         -                missing update pending
out/data/mrw_complete.csv                           -     gen_regression_vars           missing update pending
out/data/mrw_renamed.csv                            -     rename_vars                   missing update pending
\end{verbatim}

Which reveals snakemake's plan the next time its run will be to build
all outputs.

\subsection*{Exercise: Creating Cleaning
Rules}\label{exercise-creating-cleaning-rules}
\addcontentsline{toc}{subsection}{Exercise: Creating Cleaning Rules}

So far we have written a cleaning rule that deletes everything in the
\texttt{out/} directory. Construct rules that would separately clean the
\texttt{out/data/} and \texttt{out/analysis} subdirectories. Why might
we want to do this?

\chapter{Pattern Rules}\label{pattern-rules}

\section{Where we are now?}\label{where-we-are-now}

Your Snakefile should look something like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Snakemake - MRW Replication}
\CommentTok{##}
\CommentTok{## @yourname}


\CommentTok{# --- Build Rules --- #}

\KeywordTok{rule} \NormalTok{solow_intermediate:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_solow.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_intermediate.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_solow_subset_intermediate.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}

\KeywordTok{rule} \NormalTok{solow_nonoil:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_solow.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_nonoil.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_solow_subset_nonoil.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}

\KeywordTok{rule} \NormalTok{solow_oecd:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_solow.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_oecd.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_solow_subset_oecd.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}

\KeywordTok{rule} \NormalTok{gen_regression_vars:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/data-management/gen_reg_vars.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_renamed.csv"}\NormalTok{,}
        \KeywordTok{params} \NormalTok{= }\StringTok{"src/data-specs/param_solow.json"}\NormalTok{,}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{data} \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --param \{input.params\} \textbackslash{}}
\StringTok{            --out \{output.data\}"}

\KeywordTok{rule} \NormalTok{rename_vars:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/data-management/rename_variables.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"src/data/mrw.dta"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{data} \NormalTok{= }\StringTok{"out/data/mrw_renamed.csv"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --out \{output.data\}"}

\CommentTok{# --- Clean Rules --- #}
\KeywordTok{rule} \NormalTok{clean:}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"rm -rf out/*"}
\end{Highlighting}
\end{Shaded}

This is good progress, but if we look at the \emph{solow\_} rules we see
that there is quite a lot of duplication:

\begin{itemize}
\tightlist
\item
  Detail the duplication here
\end{itemize}

\section{Wildcards}\label{wildcards}

Ideally, we want our Snakefiles to feature the miniumum amount of
duplication possible (Why?). The three \emph{solow\_} rules can be
collapsed into one rule if can create a variable, for example
\texttt{iSubset}, that can iterate through the three .json files that
contain the subset filters. That is, we want to create a rule
\texttt{solow\_model} that can do the work that \texttt{solow\_nonoil},
\texttt{solow\_oecd} and \texttt{solow\_intermediate} currently do. In
Snakemake, these variables are called \emph{wildcards}.

The format of this rule will be:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{solow_model:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_solow.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_\{iSubset\}.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_solow_\{iSubset\}.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}
\end{Highlighting}
\end{Shaded}

We wrapped our wildcard \texttt{iSubset} in curly parentheses so that
Snakemake knows that we will want to substitute the name of one of the
subsets into this value. This is conceptually similar to what we have
done in our shell commands.

We can now try and run our updated Snakefile:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Building DAG of jobs...
WorkflowError:
Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards.
\end{verbatim}

What has happened? Snakemake will not execute a rule that contains
wildcards.\footnote{Technically there is another problem here too.
  Snakemake doesn't know what values to substitute into
  \texttt{iSubset.} We focus on the Wildcard error because this is what
  the Snakemake error message mentions. By fixing this error, it turns
  out we also spell out what values to substitute into \texttt{iSubset}.}
What we will do is create another rule, \texttt{run\_solow} that will
not contain wildcards \ldots{}

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{rule} \NormalTok{run_solow:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{nonoil} \NormalTok{= }\StringTok{"out/analysis/model_solow_nonoil.rds"}\NormalTok{,}
        \KeywordTok{oecd}   \NormalTok{= }\StringTok{"out/analysis/model_solow_oecd.rds"}\NormalTok{,}
        \KeywordTok{intermediate} \NormalTok{= }\StringTok{"out/analysis/model_solow_intermediate.rds"}

\KeywordTok{rule} \NormalTok{solow_model:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_solow.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_\{iSubset\}.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_solow_\{iSubset\}.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}
\end{Highlighting}
\end{Shaded}

Explain what happens here\ldots{}

There's more work we can do to reduce duplication. Look at the rule
\texttt{run\_solow}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{run_solow:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{nonoil} \NormalTok{= }\StringTok{"out/analysis/model_solow_nonoil.rds"}\NormalTok{,}
        \KeywordTok{oecd}   \NormalTok{= }\StringTok{"out/analysis/model_solow_oecd.rds"}\NormalTok{,}
        \KeywordTok{intermediate} \NormalTok{= }\StringTok{"out/analysis/model_solow_intermediate.rds"}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{The \texttt{expand()}
function}{The expand() function}}\label{the-expand-function}

Each of these inputs listed above have similar structure, with only the
name of the subset of data we are using changing. We can use another
feature of Snakemake to simplify this rule. Snakemake has an
\texttt{expand()} function that can accept a wildcard and replace it
with a set of specified values in an iterative manner. In our case, we
want to use the expand function to accept the \texttt{\{iSubset\}}
wildcard, and replace it with the values `nonoil', `oecd' and
`intermediate' one at a time. To proceed we need to do two things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a list, DATA\_SUBSETS, that contains the values we want to
  iterate through - `nonoil', `oecd' and `intermediate'.
\item
  Use Snakemake's \texttt{expand()} function to iteratively replace
  \texttt{\{iSubset\}} with each value contained in the list
  \texttt{DATA\_SUBSET}
\end{enumerate}

Let's start with (1). We will use an area above our Snakemake rules to
store the \texttt{DATA\_SUBSET} variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{DATA_SUBSET} \NormalTok{= [}
                \StringTok{"oecd"}\NormalTok{,}
                \StringTok{"intermediate"}\NormalTok{,}
                \StringTok{"nonoil"}
                \NormalTok{]}

\CommentTok{# --- Build Rules --- #}
\KeywordTok{...}
\end{Highlighting}
\end{Shaded}

The capitalization of the list DATA\_SUBSET is not essential. We do it
to separate lists that we will iterate through from other parts of our
Snakefile. This means whenever we see a capitalized name, we know it is
a list that we want to iterate through.

Next, we update the rule \texttt{run\_solow} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{run_solow:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{expand}\NormalTok{(}\StringTok{"out/analysis/model_solow_\{iSubset\}.rds"}\NormalTok{,}
                    \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET)}
\end{Highlighting}
\end{Shaded}

Now, we can clean our output folder with \texttt{\$\ snakemake\ clean}.
If we re-run snakemake we would expect everything to run. To see if this
is the case we can do a \emph{dry run}. A dry run will try go through
the Snakefile and print all the rules Snakemake wants to execute in
order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{--dryrun}
\end{Highlighting}
\end{Shaded}

which yields the following plan:

\begin{verbatim}
Building DAG of jobs...
Job counts:
    count   jobs
    1   gen_regression_vars
    1   rename_vars
    1   run_solow
    3   solow_model
    6

[Fri Jan 11 17:00:32 2019]
rule rename_vars:
    input: src/data/mrw.dta, src/data-management/rename_variables.R
    output: out/data/mrw_renamed.csv
    jobid: 5


[Fri Jan 11 17:00:32 2019]
rule gen_regression_vars:
    input: out/data/mrw_renamed.csv, src/data-management/gen_reg_vars.R, src/data-specs/param_solow.json
    output: out/data/mrw_complete.csv
    jobid: 4


[Fri Jan 11 17:00:32 2019]
rule solow_model:
    input: src/data-specs/subset_nonoil.json, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/analysis/estimate_ols_model.R
    output: out/analysis/model_solow_nonoil.rds
    jobid: 1
    wildcards: iSubset=nonoil


[Fri Jan 11 17:00:32 2019]
rule solow_model:
    input: src/data-specs/subset_oecd.json, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/analysis/estimate_ols_model.R
    output: out/analysis/model_solow_oecd.rds
    jobid: 2
    wildcards: iSubset=oecd


[Fri Jan 11 17:00:32 2019]
rule solow_model:
    input: src/data-specs/subset_intermediate.json, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/analysis/estimate_ols_model.R
    output: out/analysis/model_solow_intermediate.rds
    jobid: 3
    wildcards: iSubset=intermediate


[Fri Jan 11 17:00:32 2019]
localrule run_solow:
    input: out/analysis/model_solow_nonoil.rds, out/analysis/model_solow_oecd.rds, out/analysis/model_solow_intermediate.rds
    jobid: 0

Job counts:
    count   jobs
    1   gen_regression_vars
    1   rename_vars
    1   run_solow
    3   solow_model
    6
\end{verbatim}

This looks like what we want to happen. Hence, we re-run snakemake to
produce all output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake}
\end{Highlighting}
\end{Shaded}

\subsection*{Exercise: Exploring the expand() function
I}\label{exercise-exploring-the-expand-function-i}
\addcontentsline{toc}{subsection}{Exercise: Exploring the expand()
function I}

So far we have estimated the basic Solow model. MRW also estimate an
augmented version of the Solow model, adding human capital. The formula
required to estimate the augmented model is written up in
\texttt{src/model-specs/model\_aug\_solow.json}. Use the
\texttt{expand()} function together with the \texttt{estimate\_ols.R}
script to estimate the augmented solow model on each of the three data
subsets. The rule structures should look very similar to what we have
done so far.

\subsection*{Exercise: Exploring the expand function
II}\label{exercise-exploring-the-expand-function-ii}
\addcontentsline{toc}{subsection}{Exercise: Exploring the expand
function II}

The MRW paper contains three plots. Each of these plots use the subset
of `intermediate' countries. In the \texttt{src/figures/} subdirectory,
there are three scripts that reproduce each of the figures.\footnote{This
  is not entirely true, we are yet to figure out how to get the y-axis
  range from the original paper.} The scripts are written in such a way
that they accept exactly the same options. Using wildcards and the
expand function extend the Snakefile to construct each figure. Each
figure should be saved with the following name
`out/figures/SCRIPTNAME.pdf'

\section{Expanding Multiple
Wildcards}\label{expanding-multiple-wildcards}

The rules used to estimate the standard Solow model, and the augmented
Solow model have very similar structure:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{DATA_SUBSET} \NormalTok{= [}
                \StringTok{"oecd"}\NormalTok{,}
                \StringTok{"intermediate"}\NormalTok{,}
                \StringTok{"nonoil"}
                \NormalTok{]}

\CommentTok{# --- Build Rules --- #}
\KeywordTok{rule} \NormalTok{run_aug_solow:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{expand}\NormalTok{(}\StringTok{"out/analysis/model_aug_solow_\{iSubset\}.rds"}\NormalTok{,}
                    \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET)}

\KeywordTok{rule} \NormalTok{aug_solow_model:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_aug_solow.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_\{iSubset\}.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_aug_solow_\{iSubset\}.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}

\KeywordTok{rule} \NormalTok{run_solow:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{expand}\NormalTok{(}\StringTok{"out/analysis/model_solow_\{iSubset\}.rds"}\NormalTok{,}
                    \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET)}

\KeywordTok{rule} \NormalTok{solow_model:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_solow.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_\{iSubset\}.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_solow_\{iSubset\}.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}
\end{Highlighting}
\end{Shaded}

Add text \ldots{}

Ultimately the Snakfile becomes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= [}
          \StringTok{"solow"}\NormalTok{,}
          \StringTok{"aug_solow"}
          \NormalTok{]}

\KeywordTok{DATA_SUBSET} \NormalTok{= [}
                \StringTok{"oecd"}\NormalTok{,}
                \StringTok{"intermediate"}\NormalTok{,}
                \StringTok{"nonoil"}
                \NormalTok{]}

\CommentTok{# --- Build Rules --- #}

\KeywordTok{rule} \NormalTok{estimate_models:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{expand}\NormalTok{(}\StringTok{"out/analysis/\{iModel\}_\{iSubset\}.rds"}\NormalTok{,}
                    \KeywordTok{iModel} \NormalTok{= MODELS,}
                    \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET)}

\KeywordTok{rule} \NormalTok{ols_model:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/analysis/estimate_ols_model.R"}\NormalTok{,}
        \KeywordTok{data}   \NormalTok{= }\StringTok{"out/data/mrw_complete.csv"}\NormalTok{,}
        \KeywordTok{model}  \NormalTok{= }\StringTok{"src/model-specs/model_\{iModel\}.json"}\NormalTok{,}
        \KeywordTok{subset} \NormalTok{= }\StringTok{"src/data-specs/subset_\{iSubset\}.json"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{model_est} \NormalTok{= }\StringTok{"out/analysis/model_\{iModel\}_\{iSubset\}.rds"}\NormalTok{,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --data \{input.data\} \textbackslash{}}
\StringTok{            --model \{input.model\} \textbackslash{}}
\StringTok{            --subset \{input.subset\} \textbackslash{}}
\StringTok{            --out \{output.model_est\}"}

\KeywordTok{<...>} \CommentTok{## Other Rules below}
\end{Highlighting}
\end{Shaded}

\chapter{Automatic Wildcard
Specifications}\label{automatic-wildcard-specifications}

So far when we have wanted to expand wildcards we have manually
specified the values we want them to take. By doing so, the beginning of
our Snakefile looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= [}
          \StringTok{"solow"}\NormalTok{,}
          \StringTok{"aug_solow"}
          \NormalTok{]}

\KeywordTok{DATA_SUBSET} \NormalTok{= [}
                \StringTok{"oecd"}\NormalTok{,}
                \StringTok{"intermediate"}\NormalTok{,}
                \StringTok{"nonoil"}
                \NormalTok{]}

\KeywordTok{FIGURES} \NormalTok{= [}
            \StringTok{"aug_conditional_convergence"}\NormalTok{,}
            \StringTok{"conditional_convergence"}\NormalTok{,}
            \StringTok{"unconditional_convergence"}
            \NormalTok{]}
\end{Highlighting}
\end{Shaded}

This is not too problematic when we only have a few values that we want
the wildcard to take, but manually specifying long lists can get tedious
and is prone to error. Snakemake has a built in function,
\texttt{glob\_wildcards} that will help us to remove the manual listing
of values that we have above.

\section{\texorpdfstring{The \texttt{glob\_wildcards}
Function}{The glob\_wildcards Function}}\label{the-glob_wildcards-function}

Let's start by trying to replace the \texttt{MODELS} list that we
manually specified with a more automated approach. The
\texttt{glob\_wildcards} function takes one input - the path of the
files that we want to search combined with the part of the file name we
want to extract wrapped in parentheses and then finds all files within
that path. Thus, we will replace our original \texttt{MODELS} list with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= glob_wildcards(}\StringTok{"src/model-specs/\{fname\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To see what happens, let's add a print statement, and then execute a
dry-run:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= glob_wildcards(}\StringTok{"src/model-specs/\{fname\}"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(MODELS)}
\end{Highlighting}
\end{Shaded}

then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{--dryrun}
\end{Highlighting}
\end{Shaded}

What we are interested in is the first printed lines (in white text):

\begin{verbatim}
Wildcards(fname=['.gitkeep', 'model_solow.json', 'model_aug_cc_restr.json',
                'model_solow_restr.json', 'model_cc.json', 'model_ucc.json',
                'model_aug_solow_restr.json', 'model_aug_cc.json', 'model_aug_solow.json'])
\end{verbatim}

Here we see that all filed are returned. Compared to our original
\texttt{MODELS} list we see three differences

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are more .json files
\item
  There is a .gitkeep file
\item
  Each `fname' ends with .json, which we did't have earlier.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  is not a problem, it reflects that there is more potential analysis
  files that we havent manually specified. But (2) and (3) are
  problematic. We can remove the .gitkeep file \emph{and} the .json file
  endings with one step: telling the \texttt{glob\_wildcards} function
  to only return the part of the filename that comes before the .json.
  This will also mean that the .gitkeep is not returned, because this
  file does not have a .json ending:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= glob_wildcards(}\StringTok{"src/model-specs/\{fname\}.json"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(MODELS)}
\end{Highlighting}
\end{Shaded}

then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{--dryrun}
\end{Highlighting}
\end{Shaded}

Now the first line is:

\begin{verbatim}
Wildcards(fname=['model_solow', 'model_aug_cc_restr',
                'model_solow_restr', 'model_cc', 'model_ucc',
                'model_aug_solow_restr', 'model_aug_cc', 'model_aug_solow'])
\end{verbatim}

That's definitely an improvement. Our final step is to extract the list
called \texttt{fname} so that we can use it like our old \texttt{MODELS}
list. We do this as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= glob_wildcards(}\StringTok{"src/model-specs/\{fname\}.json"}\NormalTok{)}\KeywordTok{.fname}
\end{Highlighting}
\end{Shaded}

Now if we do a dry-run we are returned the following:

\begin{verbatim}
['model_solow', 'model_aug_cc_restr', 'model_solow_restr',
 'model_cc', 'model_ucc', 'model_aug_solow_restr',
 'model_aug_cc', 'model_aug_solow']
\end{verbatim}

which has the same structure as before.

Now if we run snakemake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake}
\end{Highlighting}
\end{Shaded}

We see that it builds the OLS estimates of the models that we have not
yet previously run\footnote{Running
  \texttt{snakemake\ estimate\_models\ -\/-force} will run all of the
  models including those that we have constructed in previous chapters.}

\subsection*{Exercise: Exploring
glob\_wildcards()}\label{exercise-exploring-glob_wildcards}
\addcontentsline{toc}{subsection}{Exercise: Exploring glob\_wildcards()}

Use the \texttt{glob\_wildcards} function to automate the construction
of the \texttt{FIGURES} list. Rememeber that depending on the order of
your Snakemake rules, you may need to explicitly call the rule that
generates figures to run the code, i.e. \texttt{snakemake\ make\_figs}.

\section{\texorpdfstring{Further Restricting the
\texttt{glob\_wildcards}
output}{Further Restricting the glob\_wildcards output}}\label{further-restricting-the-glob_wildcards-output}

So far, we have used \texttt{glob\_wildcards} output unchanged.
Sometimes the lists that it returns might have found more files than we
desire, or some of the files might be used in different ways than
others. We are now going to show a simple way to filter out unwanted
elements of the list that \texttt{glob\_wildcards} returns.

So far, we still have one list of wildcards that we have manually
specified, \texttt{DATA\_SUBSET}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= glob_wildcards(}\StringTok{"src/model-specs/\{fname\}.json"}\NormalTok{)}\KeywordTok{.fname}

\KeywordTok{DATA_SUBSET} \NormalTok{= [}
                \StringTok{"oecd"}\NormalTok{,}
                \StringTok{"intermediate"}\NormalTok{,}
                \StringTok{"nonoil"}
                \NormalTok{]}

\KeywordTok{FIGURES} \NormalTok{= glob_wildcards(}\StringTok{"src/figures/\{fname\}.json"}\NormalTok{)}\KeywordTok{.fname}
\end{Highlighting}
\end{Shaded}

Let's start by looking at the output if we run \texttt{glob\_wildcards}
as we have before:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{MODELS} \NormalTok{= glob_wildcards(}\StringTok{"src/model-specs/\{fname\}.json"}\NormalTok{)}\KeywordTok{.fname}

\KeywordTok{DATA_SUBSET} \NormalTok{= glob_wildcards(}\StringTok{"src/data-specs/\{fname\}.json"}\NormalTok{)}\KeywordTok{.fname}
\KeywordTok{print}\NormalTok{(DATA_SUBSET)}

\KeywordTok{FIGURES} \NormalTok{= glob_wildcards(}\StringTok{"src/figures/\{fname\}.json"}\NormalTok{)}\KeywordTok{.fname}
\end{Highlighting}
\end{Shaded}

and doing a dry run to examine the output of the print statement:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{--dryrun}
\end{Highlighting}
\end{Shaded}

which yields:

\begin{verbatim}
['param_solow', 'subset_oecd', 'subset_nonoil', 'subset_intermediate']
\end{verbatim}

In this list, the \texttt{param\_solow} element is the odd one out. It
is used in the data cleaning part, and does not contain a filter that
can be applied to a data frame. The question is then how do we remove
it? What we need to is complete the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  identify a pattern that we can use to filter out \texttt{param\_solow}

  \begin{itemize}
  \tightlist
  \item
    Files we want to keep all start with `subset'
  \end{itemize}
\item
  write a function that would remove it from the list

  \begin{itemize}
  \tightlist
  \item
    \texttt{lambda\ x:\ x.startswith("subset")}
  \end{itemize}
\item
  use this function to filter the out param\_solow from DATA\_SUBSET

  \begin{itemize}
  \tightlist
  \item
    \texttt{filter(lambda\ x:\ x.startswith("subset"),\ DATA\_SUBSET))}
  \end{itemize}
\item
  Take the new output as a list

  \begin{itemize}
  \tightlist
  \item
    \texttt{list(filter(lambda\ x:\ x.startswith("subset"),\ DATA\_SUBSET)))}
  \end{itemize}
\end{enumerate}

We can do this all by adding one additional line:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{DATA_SUBSET} \NormalTok{= glob_wildcards(}\StringTok{"src/data-specs/\{fname\}.json"}\NormalTok{)}\KeywordTok{.fname}
\KeywordTok{DATA_SUBSET} \NormalTok{= list(filter(lambda x: x.startswith(}\StringTok{"subset"}\NormalTok{), }\KeywordTok{DATA_SUBSET}\NormalTok{)))}
\KeywordTok{print}\NormalTok{(DATA_SUBSET)}
\end{Highlighting}
\end{Shaded}

Now again doing a dry-run with \texttt{snakemake\ -\/-dryrun} we see the
first line prints out the new DATA\_SUBSET list as:

\begin{verbatim}
['subset_oecd', 'subset_nonoil', 'subset_intermediate']
\end{verbatim}

Which is the desired output.

We can now run snakemake and see if it wants to execute any of our rules

\begin{verbatim}
snakemake --summary
\end{verbatim}

and we see that it does not. Can you explain why?

\chapter{Building all outputs at
once}\label{building-all-outputs-at-once}

In our analysis pipeline that we have constructed so far, we need to
make two calls to snakemake to run all of our analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{snakemake\ estimate\_models} - to estimate OLS regressions
\item
  \texttt{snakemake\ make\_figs} - to construct the figures
\end{enumerate}

Whilst this is not too time consuming, it would be desirable to only
have to run Snakemake once and have all our analysis in (1) and (2) be
executed. That is our next goal.

So that we are all starting from the same directory structure, let's
clean our output directory, removing all estimated models and figures:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{clean}
\end{Highlighting}
\end{Shaded}

Now if we list the contents of the \texttt{out} directory and any
potential subdirectories:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{ls} \NormalTok{-R out/}
\end{Highlighting}
\end{Shaded}

we see that there are no contents remaining:

\begin{verbatim}
out/:
\end{verbatim}

\section{\texorpdfstring{Creating an \texttt{all}
rule}{Creating an all rule}}\label{creating-an-all-rule}

Out goal is to combine all the outputs from the
\texttt{estimate\_models} and \texttt{make\_figs} rules. Let's take a
look at the structure of each of these rules so that we get a sense of
what we might need to do., Here is the \texttt{estimate\_models} rule:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{estimate_models:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{expand}\NormalTok{(}\StringTok{"out/analysis/\{iModel\}_ols_\{iSubset\}.rds"}\NormalTok{,}
                    \KeywordTok{iModel} \NormalTok{= MODELS,}
                    \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET)}
\end{Highlighting}
\end{Shaded}

And the \texttt{make\_figs} rule:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{make_figs:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{expand}\NormalTok{(}\StringTok{"out/figures/\{iFigure\}.pdf"}\NormalTok{,}
                \KeywordTok{iFigure} \NormalTok{= FIGURES)}
\end{Highlighting}
\end{Shaded}

These rules have a common structure: both having only an \texttt{input}.
From other rules that we have written, we know that a rule can have
multiple inputs if we name them. This suggests a way forward. We are
going to create a new rule called \texttt{all} that contains two inputs,
combining the inputs from the separate rules:\footnote{The name
  \texttt{all} is a convention, we could name it anything we please}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{figs}, which contains the inputs from our \texttt{make\_figs}
  rule
\item
  \texttt{models}, which contains the inputs from our
  \texttt{estimate\_models} rule
\end{enumerate}

As ever, we will have to be mindful that if we have multiple inputs,
have the trailing commas located wherever one input is followed by
another. Let's put this rule as the first in our Snakefile:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# --- Build Rules --- #}

\KeywordTok{rule} \NormalTok{all:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{figs}   \NormalTok{= expand(}\StringTok{"out/figures/\{iFigure\}.pdf"}\NormalTok{,}
                            \KeywordTok{iFigure} \NormalTok{= FIGURES),}
        \KeywordTok{models} \NormalTok{= expand(}\StringTok{"out/analysis/\{iModel\}_ols_\{iSubset\}.rds"}\NormalTok{,}
                            \KeywordTok{iModel} \NormalTok{= MODELS,}
                            \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET)}
\end{Highlighting}
\end{Shaded}

Let's check if this \texttt{all} rule does as desired - building all
outputs. If we run \texttt{snakemake\ -\/-dryrun\ all}, we get the
following output:

\begin{verbatim}
Building DAG of jobs...
Job counts:
    count   jobs
    1   all
    3   figures
    1   gen_regression_vars
    24  ols_model
    1   rename_vars
    30

[Tue Feb  5 16:23:01 2019]
rule rename_vars:
    input: src/data-management/rename_variables.R, src/data/mrw.dta
    output: out/data/mrw_renamed.csv
    jobid: 29


[Tue Feb  5 16:23:01 2019]
rule gen_regression_vars:
    input: src/data-management/gen_reg_vars.R, out/data/mrw_renamed.csv, src/data-specs/param_solow.json
    output: out/data/mrw_complete.csv
    jobid: 28


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_aug_cc_ols_subset_oecd.rds
    jobid: 1
    wildcards: iModel=model_aug_cc, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_solow_ols_subset_oecd.rds
    jobid: 2
    wildcards: iModel=model_solow, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_solow_restr_ols_subset_oecd.rds
    jobid: 4
    wildcards: iModel=model_solow_restr, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_cc_ols_subset_oecd.rds
    jobid: 5
    wildcards: iModel=model_cc, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds
    jobid: 27
    wildcards: iModel=model_aug_cc_restr, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_aug_solow_ols_subset_intermediate.rds
    jobid: 9
    wildcards: iModel=model_aug_solow, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_aug_solow_ols_subset_nonoil.rds
    jobid: 13
    wildcards: iModel=model_aug_solow, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds
    jobid: 15
    wildcards: iModel=model_aug_cc_restr, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule figures:
    input: src/figures/aug_conditional_convergence.R, out/data/mrw_complete.csv, src/data-specs/subset_intermediate.json
    output: out/figures/aug_conditional_convergence.pdf
    jobid: 18
    wildcards: iFigure=aug_conditional_convergence


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_ucc_ols_subset_oecd.rds
    jobid: 25
    wildcards: iModel=model_ucc, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_aug_solow_restr_ols_subset_oecd.rds
    jobid: 26
    wildcards: iModel=model_aug_solow_restr, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_aug_cc_restr_ols_subset_oecd.rds
    jobid: 14
    wildcards: iModel=model_aug_cc_restr, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow.json, src/data-specs/subset_oecd.json
    output: out/analysis/model_aug_solow_ols_subset_oecd.rds
    jobid: 3
    wildcards: iModel=model_aug_solow, iSubset=subset_oecd


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_solow_ols_subset_intermediate.rds
    jobid: 7
    wildcards: iModel=model_solow, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_cc_ols_subset_nonoil.rds
    jobid: 23
    wildcards: iModel=model_cc, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_solow_restr_ols_subset_intermediate.rds
    jobid: 10
    wildcards: iModel=model_solow_restr, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_aug_cc_ols_subset_intermediate.rds
    jobid: 11
    wildcards: iModel=model_aug_cc, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
rule figures:
    input: src/figures/unconditional_convergence.R, out/data/mrw_complete.csv, src/data-specs/subset_intermediate.json
    output: out/figures/unconditional_convergence.pdf
    jobid: 24
    wildcards: iFigure=unconditional_convergence


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_cc_ols_subset_intermediate.rds
    jobid: 8
    wildcards: iModel=model_cc, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_solow_restr_ols_subset_nonoil.rds
    jobid: 12
    wildcards: iModel=model_solow_restr, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_solow_ols_subset_nonoil.rds
    jobid: 16
    wildcards: iModel=model_solow, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_aug_cc_ols_subset_nonoil.rds
    jobid: 17
    wildcards: iModel=model_aug_cc, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule figures:
    input: src/figures/conditional_convergence.R, out/data/mrw_complete.csv, src/data-specs/subset_intermediate.json
    output: out/figures/conditional_convergence.pdf
    jobid: 19
    wildcards: iFigure=conditional_convergence


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_ucc_ols_subset_intermediate.rds
    jobid: 20
    wildcards: iModel=model_ucc, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds
    jobid: 21
    wildcards: iModel=model_aug_solow_restr, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_nonoil.json
    output: out/analysis/model_ucc_ols_subset_nonoil.rds
    jobid: 22
    wildcards: iModel=model_ucc, iSubset=subset_nonoil


[Tue Feb  5 16:23:01 2019]
rule ols_model:
    input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_intermediate.json
    output: out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds
    jobid: 6
    wildcards: iModel=model_aug_solow_restr, iSubset=subset_intermediate


[Tue Feb  5 16:23:01 2019]
localrule all:
    input: out/figures/conditional_convergence.pdf, out/figures/unconditional_convergence.pdf, out/figures/aug_conditional_convergence.pdf, out/analysis/model_solow_ols_subset_oecd.rds, out/analysis/model_aug_cc_restr_ols_subset_oecd.rds, out/analysis/model_solow_restr_ols_subset_oecd.rds, out/analysis/model_cc_ols_subset_oecd.rds, out/analysis/model_ucc_ols_subset_oecd.rds, out/analysis/model_aug_solow_restr_ols_subset_oecd.rds, out/analysis/model_aug_cc_ols_subset_oecd.rds, out/analysis/model_aug_solow_ols_subset_oecd.rds, out/analysis/model_solow_ols_subset_nonoil.rds, out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds, out/analysis/model_solow_restr_ols_subset_nonoil.rds, out/analysis/model_cc_ols_subset_nonoil.rds, out/analysis/model_ucc_ols_subset_nonoil.rds, out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds, out/analysis/model_aug_cc_ols_subset_nonoil.rds, out/analysis/model_aug_solow_ols_subset_nonoil.rds, out/analysis/model_solow_ols_subset_intermediate.rds, out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds, out/analysis/model_solow_restr_ols_subset_intermediate.rds, out/analysis/model_cc_ols_subset_intermediate.rds, out/analysis/model_ucc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds, out/analysis/model_aug_cc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_ols_subset_intermediate.rds
    jobid: 0

Job counts:
    count   jobs
    1   all
    3   figures
    1   gen_regression_vars
    24  ols_model
    1   rename_vars
    30
\end{verbatim}

The output shows that we have achieved our desired goal. The
\texttt{all} rule builds all outputs. Notice that if you examine the
order that Snakemake plans to execute the jobs that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First the data management steps are executed to prepare data

  \begin{itemize}
  \tightlist
  \item
    This is what we would expect, as we cannot do any analysis without
    cleaned data
  \end{itemize}
\item
  The order in which the figures and regressions are executed are mixed
  together

  \begin{itemize}
  \tightlist
  \item
    This is because the figures and regression outputs do not depend on
    each other
  \end{itemize}
\end{enumerate}

Now we build our entire project\footnote{Notice that we placed the
  \texttt{all} rule as the first rule in our Snakefile. Snakemake's
  default behaviour is to build the first rule in the file, so we could
  instead type only \texttt{snakemake} and get the same result.}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{all}
\end{Highlighting}
\end{Shaded}

\chapter{Adding Parameters}\label{adding-parameters}

In our analysis pipeline so far we have always passed files to our R
scripts. While we promote this practice, and wrapping important
information into .json files, sometimes it seems like overkill to write
a new json file to contain one line of configuration. An alternative to
passing these json files is to use Snakemake's built in \texttt{params}
arguments, which are rule specific, to store information that we want to
pass to our R Script. This goal of this chapter is to show how to use
\texttt{params} to pass across a piece of information.

\section{Motivating Example: Constructing a Regression Table from OLS
Results}\label{motivating-example-constructing-a-regression-table-from-ols-results}

So far, we have estimated a series of OLS regressions and stored there
output inside the \texttt{out/analysis} directory. Typically once we
have estimated one or more models, we want to format the output into a
regression table that we can insert into a written document like a paper
or set of presentation slides. In the folder \texttt{src/tables/} we can
see that there are a series of R scripts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{ls} \NormalTok{src/tables/}
\end{Highlighting}
\end{Shaded}

which prints to the screen:

\begin{verbatim}
tab01_textbook_solow.R  tab03_ucc_solow.R  tab05_cc_aug_solow.R
tab02_augment_solow.R   tab04_cc_solow.R   tab06_cc_aug_solow_restr.R
\end{verbatim}

This shows that the example is designed to build 6 tables. Each table
has it's own script that constructs it. We will start by constructing
Table 1, from \texttt{tab01\_textbook\_solow.R} Let's have a look at
what information this script expects us to pass using the help flag:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{Rscript} \NormalTok{src/tables/tab01_textbook_solow.R --help}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Usage: src/tables/tab01_textbook_solow.R [options]


Options:
    -fp CHARACTER, --filepath=CHARACTER
        A directory path where models are saved

    -m CHARACTER, --models=CHARACTER
        A regex of the models to load

    -o CHARACTER, --out=CHARACTER
        output file name [default = out.tex]

    -h, --help
        Show this help message and exit
\end{verbatim}

From this we learn that we need to pass:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{-\/-filepath}, which is the directory where our OLS models are
  stored
\item
  \texttt{-\/-models}, a regular expression to tell R which models
  within the filepath to workwith
\item
  \texttt{-\/-out}, a .tex file where we want to direct the output
\end{enumerate}

Now we will work on constructing this rule.

\section{\texorpdfstring{Creating a Rule with
\texttt{params}}{Creating a Rule with params}}\label{creating-a-rule-with-params}

We are going to use the \texttt{params} option to pass across the
filepath and the models regular expression into R. A sketch of the rule
we want to create is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{textbook_solow:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= ,}
        \KeywordTok{models} \NormalTok{=}
    \KeywordTok{params}\NormalTok{:}
        \KeywordTok{filepath}   \NormalTok{= ,}
        \KeywordTok{model_expr} \NormalTok{=}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{table} \NormalTok{= ,}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --filepath \{params.filepath\} \textbackslash{}}
\StringTok{            --models \{params.model_expr\} \textbackslash{}}
\StringTok{            --out \{output.table\}"}
\end{Highlighting}
\end{Shaded}

There are two important points to notice about how we added params to
our rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{params} are added to the rule in a similar way to inputs and
  lists
\item
  \texttt{params} are referenced identically to inputs and outputs in
  the shell command
\end{enumerate}

Now we need to decide what information needs to be entered into each
line of our rule. EXPLAIN

Our rule then becomes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{textbook_solow:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{script} \NormalTok{= }\StringTok{"src/tables/tab01_textbook_solow.R"}\NormalTok{,}
        \KeywordTok{models} \NormalTok{= expand(}\StringTok{"out/analysis/\{iModel\}_ols_\{iSubset\}.rds"}\NormalTok{,}
                            \KeywordTok{iModel} \NormalTok{= MODELS,}
                            \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET)),}
    \KeywordTok{params}\NormalTok{:}
        \KeywordTok{filepath}   \NormalTok{= }\StringTok{"out/analysis/"}\NormalTok{,}
        \KeywordTok{model_expr} \NormalTok{= }\StringTok{"model_solow*.rds"}
    \KeywordTok{output}\NormalTok{:}
        \KeywordTok{table} \NormalTok{= }\StringTok{"out/tables/tab01_textbook_solow.tex"}
    \KeywordTok{shell}\NormalTok{:}
        \StringTok{"Rscript \{input.script\} \textbackslash{}}
\StringTok{            --filepath \{params.filepath\} \textbackslash{}}
\StringTok{            --models \{params.model_expr\} \textbackslash{}}
\StringTok{            --out \{output.table\}"}
\end{Highlighting}
\end{Shaded}

There are two ways to run this rule:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tell snakemake to run this rule explicitly,
  \texttt{snakemake\ textbook\_solow}

  \begin{itemize}
  \tightlist
  \item
    Because it is not the first rule of the Snakefile it isnt run by
    default
  \end{itemize}
\item
  Add the output of this rule to the \texttt{all} rule.

  \begin{itemize}
  \tightlist
  \item
    Adds creating this table to our complete analysis pipeline
  \end{itemize}
\end{enumerate}

We prefer (2). Hence we also update the \texttt{all} rule as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rule} \NormalTok{all:}
    \KeywordTok{input}\NormalTok{:}
        \KeywordTok{figs}   \NormalTok{= expand(}\StringTok{"out/figures/\{iFigure\}.pdf"}\NormalTok{,}
                            \KeywordTok{iFigure} \NormalTok{= FIGURES),}
        \KeywordTok{models} \NormalTok{= expand(}\StringTok{"out/analysis/\{iModel\}_ols_\{iSubset\}.rds"}\NormalTok{,}
                            \KeywordTok{iModel} \NormalTok{= MODELS,}
                            \KeywordTok{iSubset} \NormalTok{= DATA_SUBSET),}
        \KeywordTok{tab01}  \NormalTok{= }\StringTok{"out/tables/tab01_textbook_solow.tex"}
\end{Highlighting}
\end{Shaded}

If we then do a dry run to see what Snakemake plans to do\footnote{An
  alternative would be to run \texttt{snakemake\ -\/-summary} and
  examine the output.}:

\begin{verbatim}
Building DAG of jobs...
Job counts:
    count   jobs
    1   all
    1   textbook_solow
    2

[Tue Feb  5 17:42:59 2019]
rule textbook_solow:
    input: src/tables/tab01_textbook_solow.R, out/analysis/model_solow_ols_subset_oecd.rds, out/analysis/model_solow_ols_subset_nonoil.rds, out/analysis/model_solow_ols_subset_intermediate.rds, out/analysis/model_aug_cc_restr_ols_subset_oecd.rds, out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds, out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds, out/analysis/model_solow_restr_ols_subset_oecd.rds, out/analysis/model_solow_restr_ols_subset_nonoil.rds, out/analysis/model_solow_restr_ols_subset_intermediate.rds, out/analysis/model_cc_ols_subset_oecd.rds, out/analysis/model_cc_ols_subset_nonoil.rds, out/analysis/model_cc_ols_subset_intermediate.rds, out/analysis/model_ucc_ols_subset_oecd.rds, out/analysis/model_ucc_ols_subset_nonoil.rds, out/analysis/model_ucc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_restr_ols_subset_oecd.rds, out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds, out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds, out/analysis/model_aug_cc_ols_subset_oecd.rds, out/analysis/model_aug_cc_ols_subset_nonoil.rds, out/analysis/model_aug_cc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_ols_subset_oecd.rds, out/analysis/model_aug_solow_ols_subset_nonoil.rds, out/analysis/model_aug_solow_ols_subset_intermediate.rds
    output: out/tables/tab01_textbook_solow.tex
    jobid: 27


[Tue Feb  5 17:42:59 2019]
localrule all:
    input: out/figures/conditional_convergence.pdf, out/figures/unconditional_convergence.pdf, out/figures/aug_conditional_convergence.pdf, out/analysis/model_solow_ols_subset_oecd.rds, out/analysis/model_solow_ols_subset_nonoil.rds, out/analysis/model_solow_ols_subset_intermediate.rds, out/analysis/model_aug_cc_restr_ols_subset_oecd.rds, out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds, out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds, out/analysis/model_solow_restr_ols_subset_oecd.rds, out/analysis/model_solow_restr_ols_subset_nonoil.rds, out/analysis/model_solow_restr_ols_subset_intermediate.rds, out/analysis/model_cc_ols_subset_oecd.rds, out/analysis/model_cc_ols_subset_nonoil.rds, out/analysis/model_cc_ols_subset_intermediate.rds, out/analysis/model_ucc_ols_subset_oecd.rds, out/analysis/model_ucc_ols_subset_nonoil.rds, out/analysis/model_ucc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_restr_ols_subset_oecd.rds, out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds, out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds, out/analysis/model_aug_cc_ols_subset_oecd.rds, out/analysis/model_aug_cc_ols_subset_nonoil.rds, out/analysis/model_aug_cc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_ols_subset_oecd.rds, out/analysis/model_aug_solow_ols_subset_nonoil.rds, out/analysis/model_aug_solow_ols_subset_intermediate.rds, out/tables/tab01_textbook_solow.tex
    jobid: 0

Job counts:
    count   jobs
    1   all
    1   textbook_solow
    2
\end{verbatim}

We see that snakemake only needs to create the table from our newly
created rule. Now run snakemake to build the table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{snakemake} \NormalTok{all}
\end{Highlighting}
\end{Shaded}

and when finished if we list the contents of \texttt{out/tables} we see
our new regression table has been created:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{ls} \NormalTok{out/tables/}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tab01_textbook_solow.tex
\end{verbatim}

\subsection*{Exercise: Building Table
2}\label{exercise-building-table-2}
\addcontentsline{toc}{subsection}{Exercise: Building Table 2}

Using the same rule format as above, incorporate \texttt{params} into a
new rule called \texttt{augment\_solow} that constructs Table 2.

\chapter*{PART II}\label{part-ii}
\addcontentsline{toc}{chapter}{PART II}

\chapter{Logging Output and Errors}\label{logging-output-and-errors}

Content is TBD

\chapter{Self Documenting Help}\label{self-documenting-help}

Content is TBD

\chapter{Config Files}\label{config-files}

Content is TBD

\chapter{Subworkflows: Divide and
Conquer}\label{subworkflows-divide-and-conquer}

Content is TBD

\chapter*{PART III}\label{part-iii}
\addcontentsline{toc}{chapter}{PART III}

\chapter{Reproducible Articles with
Rmd}\label{reproducible-articles-with-rmd}

Content is TBD

\chapter{Reproducible Slides with
Rmd}\label{reproducible-slides-with-rmd}

Content TBD

\chapter{Package Dependencies with
Packrat}\label{package-dependencies-with-packrat}

\chapter{Containers}\label{containers}

\bibliography{references.bib}


\end{document}
