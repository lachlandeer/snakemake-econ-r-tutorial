[["index.html", "Reproducible Data Analytic Workflows with Snakemake and R An Extended Tutorial for Researchers in Business, Economics and the Social Sciences Preface Why we wrote this tutorial Pre-requisite Knowledge What Needs to be Downloaded &amp; Installed for this Tutorial What We Can’t Cover Acknowledgements About the Authors License Citation", " Reproducible Data Analytic Workflows with Snakemake and R An Extended Tutorial for Researchers in Business, Economics and the Social Sciences Ulrich Bergmann Lachlan Deer Julian Langer This Version: 2021-02-22 – Draft Under Active Development Preface Why we wrote this tutorial Empirical research workflows in economics and other social sciences typically take the form of a set of steps that need to be performed in a given order. For example, one may need to run a set of data cleaning and merging scripts one after the other before creating a set of summary statistics and figures, and then running some empirical modelling such as a series of linear regressions. These linear regression results need to be compiled into tables, and these tables need to be included in a research paper and a slide deck. The number of steps involved in completing an entire project from initial data preparation, through to analysis and compiling the final results in a document quickly becomes large, and the inter-relationships between steps complex. Our own experience, and discussions with colleagues suggest that such a workflow quickly becomes unwieldy, being both difficult to remember the relationships between steps and hard to write down in a way that co-authors and our future selves can access without blood pressures &amp; stress levels rising. (the latter, at least from our personal experience stems from a degree of laziness to document our steps after we have completed them). We found ourselves frustrated with the headaches manual management of our workflow created and searched for better ways of managing our research workflow. This tutorial introduces the Snakemake workflow management system as a way to execute research workflows and manage the dependencies between the succesive steps that make up our workflow. We have found the Snakemake system to be the best solution to our various workflow needs across our own research in business, economics and social science due to a combination of its’ readabilty and the ease of scaling up as the project becomes increasingly complex. Intuitively you can think of Snakemake workflows as a set of of human readable steps designed to create a replicable analyses: We the researcher will need to write down each step in the form of a ‘recipe’ - what goes in, what comes out, and how do we combine the inputs to produce an output. Once we have written these steps, Snakemake will execute them for us and track the dependencies between steps. This means if we update step \\(x\\), for example by changing how we merge data, the . Snakemake will then look for other steps in our workflow that would use this updated data, and re-execute them, and any steps further down our analysis that may use this next set of updated outputs.1 Our hope is that the pages that follow provide a gentle introduction to Snakemake and replicable, automated workflow management. The tutorial assumes no knowledge of Snakemake or other workflow management software. Each chapter aims to build up one’s knowledge through the means of a guided example combined with some exercises and solutions. Our aim is to build up a working understanding of how to use the software so that at the end of the tutorial the reader could get a new or existing project up and running using the Snakemake paradigm with minimal additional overheads. While this tutorial is targeted at researchers who are considering adopting Snakemake for the first time, we hope it can also serve as a useful reference for people are have some working knowledge of Snakemake. We often come back to the material ourselves to find snippets of code to include and adapt into our own research workflows and anticipate others may do the same. Pre-requisite Knowledge To successfully work through this tutorial you will need some programming background with R, python and the command line. We have found that familiarity with the following concepts puts you in good stead to understand what follows: Working Knowledge of R Including writing command line programs, and Dynamic Reports with knitr and rmarkdown For a quick review see Software Carpentry’s ‘Programming with R’ Lesson Basic Knowledge of Python (Snakemake is written in Python) Data Types, variable creation, lists and dictionaries For a quick review see sections 1, 5 and 12 of of Software Carpentry’s ‘Programming with Python’ Lesson Basic Working Knowledge of a bash terminal File system structure, Changing directories, Making directories, Removing Files and Directories, Running a script from command line For a quick review see sections 1, 2, 3 and 6 of Software Carpentry’s ‘Unix Shell’ Lesson What Needs to be Downloaded &amp; Installed for this Tutorial Template Scripts and Documents The backbone of our tutorial is a dataset and a set of R scripts that need to be executed in a particular order, and two Rmarkdown documents which have template versions of a paper and slides. To approximate a larger research workflow, some scripts will need to be run multiple times across different configurations. To work through the tutorial - building up your own Snakemake workflow and running the commands yourself on your own computer, you will need to download these scripts and markdown documents. We provide two ways to download these files: Download a zip archive here. After downloading, proceed to unzip the folder and move it to a location in your computer that you feel comfortable to work with.2 Download the template scripts using Git: git clone https://github.com/lachlandeer/snakemake-econ-r-learner.git !! TIP !! When we have taught this tutorial live in person in the past, we have created a folder on each learner’s computer which we tell them to think of as a “safe space” - where we can’t accidently move, delete or otherwise modify any files on their computer they deem important by accident. For example, if learners have a ‘coursework’ folder, we ask them to create a new sub-folder called “20YY-MM-programming class”, where YY and MM are the year and month we are giving the class. We then ask them to download this folder / clone the repository inside this space. Software Installation In addition to the R scripts and data, you will need to have some software installed on your computer. You need to have the following installed: Access to a bash style terminal Python 3 (Python 3.6 or higher) Snakemake R (version 4.0.x) Some R packages for additional functionality We provide instructions below. Access to a bash style terminal Windows Users Windows does not have a bash terminal installed by default. Follow instructions here to install one. Mac Users You have a bash style terminal installed by default. To open a terminal session: Open spotlight with cmd + space Type in ‘terminal’ When the terminal appears, open it. Linux Users You have a bash style terminal installed by default. Instructions to open a terminal vary across Linux flavours. On Ubuntu style flavours, Ctrl + Alt + T will work. Installing Python Either: Install Anaconda Python (all OS): We provide instructions on how to install anaconda python here Install Python using the deadsnakes ppa (Ubuntu Specific): Here’s how to add the deadsnakes ppa and install Python 3.8 $ sudo apt-get install software-properties-common $ sudo add-apt-repository ppa:deadsnakes/ppa $ sudo apt-get update $ sudo apt-get install python3.8 Installing Snakemake Inside your terminal window enter the following command and press Return pip3 install snakemake you may need to replace pip3 with pip Windows Users: If you get an installation error here, the the following commands sequentially: pip install datrie pip install snakemake If you are still having issues: If you are completing this tutorial as part of a live workshop, raise your hand Else, create an issue here, and tell us what the error message is + what operating system you are running on. We will try and resolve it. Installing R We provide instructions on how to install R here Installing the Required R libraries We utilize some additional R packages inside the scripts that build our project. To install either: If you have RStudio and are comfortable installing packages there. Open Rstudio Copy and paste the following lines into the R console: to_install &lt;- c(&quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;haven&quot;, &quot;magrittr&quot;, &quot;optparse&quot;, &quot;purrr&quot;, &quot;readr&quot;, &quot;rjson&quot;, &quot;rlist&quot;, &quot;stargazer&quot;, &quot;tibble&quot;, ) install.packages(to_install, repos = &quot;https://cloud.r-project.org/&quot;) A command line alternative: Open your terminal and change directory to the folder called snakemake-econ-r-learner you downloaded in a previous step cd PATH/TO/snakemake-econ-r-learner Run type the following command into the terminal and press Return: Rscript install_r_packages.R What We Can’t Cover Our goal of providing a gentle introduction to workflow management via Snakemake means that we have deliberately sacrificed details and . In particular, we do not cover: A detailed discussion about the inner workings of Snakemake A wide-ranging introduction to all of Snakemake’s features (which continue to grow) If you are interested in either of these, we recommend you look at the Snakemake documentation (LINK) and the supporting technical publication (LINK). We have found each of these references are extremely useful - but quite dense and difficult to process for new-comers. Acknowledgements First and foremost, we would like to thank the Department of Economics at the University of Zurich for giving us the opportunity to The idea of This tutorial is a extension of the idea and implementation of reproducible workflow management that we taught during these classes. We are indebted to the 2016 - 2020 cohorts of the PhD and research assistants who Their questions, confused looks and XX have all improved We would like to thank our colleagues and friends who co-taught the Programming Practices classes at Lachlan and Julian were introduced to the concept of workflow management software by Hans-Martin von Gaudecker during a short course on Effective Programming Practices in 20XX. The introduction to Hans-Martin’s Templates for “Reproducible Research Projects in Economics” using Waf and (now) pytask provided much inspiration for the structure of our own template and this tutorial. About the Authors Ulrich Bergmann is … Lachlan Deer (Twitter handle: lachlandeer) is an Assistant Professor of Marketing at Tilburg University, a Fellow at the University of Zurich’s Centre for Reproducible Science and an instructor and lesson maintainer for the Carpentries Project. Prior to joining Tilburg University he was a Postdoctoral Fellow at the University of Chicago Booth School of Business, and completed his PhD in Economics at the University of Zurich in 2019. His substantive research interests lie in social media and it’s impact on media and entertainment industries, and the role of social media networks in political revolutions. From 2016 he is co-lead at of the “Programming Practices for Research in Economics” inititative teaching coding skills and reproducible research principles to early career researchers in economics and business. In his spare time he enjoys strumming his ukelele, improving his caffe latte making skills and building lego models. Julian Langer (Twitter handle: jlnlanger) is a Fellow at the Access to Justice Lab at Harvard Law School and a visiting researcher at the Berlin Social Science Center (WZB). He completed his PhD in Economics at the University of Zurich in July 2020. His substantive research interests lie in economic history, political economy, the economics of crime, and text-as-data. Since 2016 he co-leads the “Programming Practices for Research in Economics” initiative, teaching coding skills and reproducible research principles. When not at work he enjoys… License Instructional material All instructional material made available under the tutorial titled “Reproducible Data Analytic Workflows with Snakemake and R” and authored by Ulrich Bergmann, Lachlan Deer and Julian Langer is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license. You are free: to Share—copy and redistribute the material in any medium or format to Adapt—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit (mentioning that your work is derived from this tutorial and, where practical, linking to the material http://lachlandeer.github.io/snakemake-econ-r-tutorial), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. With the understanding that: Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. Software Except where otherwise noted, the example programs and other software provided by Ulrich Bergmann, Lachlan Deer and Julian Langer are made available under the OSI-approved MIT license. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Citation Please cite as: Bergmann, U, Deer, L and Langer, J. (2021, February). “Reproducible Data Analytic Workflows with Snakemake and R: An Extended Tutorial for Researchers in Business, Economics and the Social Sciences”. February 2021 (Version v2021.1.0) https://lachlandeer.github.io/snakemake-econ-r-tutorial @misc{bdl2020snakemakeecon, title={Reproducible Data Analytic Workflows with Snakemake and `R`: An Extended Tutorial for Researchers in Business, Economics and the Social Sciences}, author={Ulrich Bergmann and Lachlan Deer and Julian Langer}, year={2021}, url = &quot;https://lachlandeer.github.io/snakemake-econ-r-tutorial&quot; } If this idea of inter-related steps feels unfamilar at this stage, we will formalize this idea in the pages that follow. ↩︎ For example, you could move it your , or your Desktop.↩︎ "],["part-i-preliminaries.html", "PART I - Preliminaries", " PART I - Preliminaries "],["intro.html", "Chapter 1 Motivating &amp; Rationale 1.1 A Case for Reproducibility 1.2 What is Snakemake &amp; Why Should you use it? 1.3 Why R? 1.4 Working Example: Replicating Mankiw, Romer and Weil’s 1992 QJE 1.5 The way forward", " Chapter 1 Motivating &amp; Rationale 1.1 A Case for Reproducibility 1.1.1 How far to go in the quest for reproducibility? 1.2 What is Snakemake &amp; Why Should you use it? 1.3 Why R? 1.4 Working Example: Replicating Mankiw, Romer and Weil’s 1992 QJE Throughout our tutorial we are going to use a running example to illustrate the concepts we discuss. 1.5 The way forward For the purpose of this tutorial we will focus on replicating the following aspects of the MRW paper:3 Regression Tables 1 and 2: Estimating the Textbook- and Augmented Solow Model Figure 1: Unconditional Versus Conditional Convergence To replicate these we will need to proceed as follows: Perform some data management Prepare the data before we run regressions Do some analysis. For example, run regressions for: Different subsets of data Alternative econometric specifications Turn the statistical output of the regressions into a tabular format that we can insert into a document Construct a set of graphs Integrate the tables and graphs into a paper and a set of slides (optional) We hope that these 5 steps look familiar - as they were designed to represent a simplifed workflow for an applied economist or social science researcher. Before proceeding to understanding how to use Snakemake and R to construct a reproducible workflow, the next chapter first takes a deeper dive into the a protypical way to set up a research project on our computer. Exercise: Your own project’s steps Think about a project you are working on or have worked on in the past (it may be a Bachelor or Master’s thesis or a recent / active research project). Does your project fit into the 5 steps we described above? If not, what would you modify or add to our 5 steps? (Do you think this would destroy the general principles we will encourage over the next chapters?) A complete replication using the concepts presented in this tutorial is available here↩︎ "],["project-organization.html", "Chapter 2 Project Organization Learning Objectives 2.1 Subtitle TBD 2.2 Project Organization I: Separating Inputs and Outputs 2.3 Project Structure II: Separating Logical Chunks of the Project 2.4 Project Structure III: Separating Input Parameters from Code", " Chapter 2 Project Organization Learning Objectives TODO: Turn these into actual goals Separate inputs and outputs Separate directories by function Minimize hard-coded information inside scripts 2.1 Subtitle TBD As researchers and students we are famililar with the notion a paper or report needs to be written in a clean and organized way that can be understood by ourselves and others. Papers and reports have a logical structure, and are broken into logical chunks. When the conclusions are driven by assumptions or … we try to make this explicit by highlighting where they are or having a separate section to introduce them. When logic is unclear and assumptions are not clearly laid out, we are encouraged to organize written our arguments in a better way. The idea of maintaining a clean and organized structure often does not extend to our thinking about the code and data that are used to construct the paper’s main results. Despite our best intentions to keep our code and data organized and coherent as researcher, it too often becomes one of the first principles we throw out - either when the complexity grows too large, or when looming deadlines favour the production of results rather than how clean the scripts that produce them look.4 Until recently, our paper’s “backend” of code and data is typically only visible to the authors of a project, meaning any frustration and difficulties brought about by bad code and data organization fell only on us, the authors. The rise of open science and reproducibility requirements by publishers have started to open up these backends, increasing the need for our code and data to be structured in a way that that is amenable to others to look at and understand. As a result, the pressure to adopt a coherent way to organize our research project’s files has risen beyond keeping the frustations of our coauthors and our future-selves in check, demanding a solution. This aim of this chapter is to provide an example structure for a research project and explain the logic behind the structure we propose. There are two core principles behind the way we organize the files for a project: (1) the separation of logical chunks of a project’s code and data, and (2) the separation of parameters, specifications and paths from code. A side benefit of both of these principles is that our project’s structure becomes portable across multiple research projects. This means that once we make a decision to adopt a structure, we can continue to use the same structure repeatedly into the future.5 We are going to use the files and structure of the folder you have downloaded that replicates Mankiw, Romer and Weil’s analysis (hereafter MRW) to look explore one way to organize a research project.6 The organization of code and data that you will see emphasizes the notions of separating logical chunks of code, data, file paths &amp; specifications, along with the files that the statistical analysis that R produces. As you work through what follows you may begin to question the extent of logical separation, and whether it is a step too far. If this is where your thoughts start to go, also reflect on how this structure could be extended if this project were to develop further and on how readily it could be adopted for use in a new project. 2.2 Project Organization I: Separating Inputs and Outputs The first step in our ‘logical separation journey’ is the organization of files and folders. Starting a project with the right folder structure, even before a line of code is written, is an easy starting point and will help us stay on track as the project develops. Let’s take a look at the main folder structures present in our MRW project. To get started, we need to “open up” the folder to inspect what is there. As with the remainder of this tutorial - we will use the terminal to interact with our computer, so “opening” a folder means we will change into that directory. Thus, open a terminal7 and change directory to the MRW project directory as follows: $ cd YOUR/PATH/TO/snakemake-econ-r-learner You can verify you have done this correctly by entering the pwd command into the terminal and hitting the RETURN key. The expected output will be: YOUR/PATH/TO/snakemake-econ-r-learner Now we want to look at the files and folders present in that directory: $ ls We see the following: ./ |- src/ |- out/ |- log/ |- sandbox/ README.md Snakefile The main idea here is that there is a top level directory ./ which contains four sub-folders: src, out, log and sandbox. In addition to these subfolders, there is two files, README.md and Snakefile. Importantly there is no R scripts or data files in this top level directory. We briefly discuss each of these in turn. Root Folder (./). This is the main folder of the project. Everything that is used or produced for the project should be located in a this folder, or better yet a sub-folder. Think of it as a ‘one-stop shop’ for what you are working on for this project. If it’s important - you must be able to find it here. src folder. This is the ‘source’ folder. It contains all of your code files you develop during your analysis and the original datasets you begin your analysis with. out folder. This is the output directory. We will put anything that we create by running a R script. For example, it can contain new datasets we create by cleaning and merging our original data, saved regression results and saved figures or summary tables. The main point is that anything we can recreate by running R scripts will get saved here - whether they be ‘temporary’ or ‘intermediate’ outputs that will get used by another R script later in our project, or ‘final’ outputs that we will want to insert into a paper, report or slide deck. log folder. When we run and R script it produces output and messages, typically printing them to the screen. If we want to save those messages to a file, so we can refer to them in the future, we will save them here. sandbox folder. As we work through our project, we will want to explore new ideas and test out how to code them. While we play with these bits and pieces of code, we save them in the sandbox. Separating them from src means we know that R scripts in here are ‘under development’. When they are finalized, we can move them into src. README.md file. A README is a text file that introduces and explains the project. We should write information that explains what the project is about and how someone can run the scripts developed for the project in this file. We also recommend providing installation instructions to clarify exactly what needs to be installed to run the project. Snakefile. We will use the Snakefile to write the steps needed that run all the scripts in the project. Further discussion of Snakefiles are deferred to the next chapter when introduce Snakemake. The advantage of following this folder structure means that anyone who looks through the your code can easily figure out what the original/raw data sets are and what files need to be executed versus what files have been created by R scripts. This is beneficial not only to others trying to understand your code, but also future versions of yourself when you come back to your project’s code after not working on it for a while. 2.3 Project Structure II: Separating Logical Chunks of the Project Now that we are looking to keep our project’s structure clean, we want to keep all the computer code inside the src directory. However if we were to simply put all our computer code and data into src although we solve part of our project disorganization, we still potentially have many original datasets and many R scripts all located in one directory. This half-organized - half-disorganized approach is definitely an improvement of having all our files located in one place, but one can (and should) do better. To keep our project even more organized, we create a set of subdirectories inside src to separate logical chunks of the project. Let’s have a look at the content of src. $ ls src/ We see the following output: ./ |src/ |- data/ |- data-management/ |- data-specs/ |- analysis/ |- model-specs/ |- lib/ |- figures/ |- tables/ |- table-specs/ |- paper/ |- slides/ Here we see 9 sub-folders inside src. These are designed to logically separate various aspects of the project: data/ contains all of the project’s original/raw data. data-management/ contains all R scripts to clean and merge datasets together data-specs/ contains any special parameterizations used in cleaning or analysis. analysis/ contains all R scripts that are our main analysis. For example, our regression scripts model-specs/ conatins lib/ contains R scripts that contain functions that can be used more generally. For example helper functions that can be used in both data cleaning and analysis could be put here. So can scripts that contain functions that can be portable across multiple projects. figures/ contains R scripts that produce figures. One script per figure. tables/ contains R scripts that produce summary tables and regression tables. One script per table. table-specs - contains the paper contains the Rmarkdown files to write up project results in a paper, i.e. the paper’s text. slides contains the Rmarkdown files to write up project results as a slide deck, i.e. the text of the slides. This separation begins to make clear the logical steps to produce a project which is useful in itself. There’s some orginal data set in data that needs to be tidied in some way. The scripts in data-management do that cleaning. Once the data is cleaned, scripts in analysis perform the statistical analysis on clean data. Summary figures and tables, and processed results from the analysis come from the scripts inside tables and figures. Finally, paper and slides contain source files for the writeup and presentation of results. Although we are yet to tell anyone how to use these scripts in the sub-folders, the organization of the src folder is already extremely suggestive about the steps in our project. !!IN A WARNING BOX!! The presence of the folders data-specs, model-specs and table-specs might still seems a little confusing. We use these folders and the files within them to separate out data and model parameterizations - which can be thought of as a further level of project organization. We will return to this idea in section XX. In the same way that we have sub-folders in src, we also want any of the outputs we produce to organized inside the out directory. Using a similar structure between src and out is a good way to do this. Let’s look at the structure of out: $ ls out/ ./ |out/ |- data/ |- analysis/ |- figures/ |- tables/ |- paper/ |- slides/ We have a sub-folder inside out for each sub-folder in src that has a step in our project: out/data can store any datasets that are produced as a result of cleaning and merging out/analysis can store any output from statistical analysis, for example saved regression results out/figures holds all figures produced out/tables holds all tables produced out/paper has the pdf of the paper out/slides has the pdf of the slides 2.3.1 Aside: Even Deeper Sub-folders If we look inside the src/data directory $ ls -F src/data/ We see one data file: mrw.dta That is, our data/ directory contains the project’s original data set. There is only one data set because the project is relatively simple - and only relies on one source of data In more extensive projects, the data/ subfolder would typically have more than one data set. For example: dataset1.dta dataset2.dta dataset3.csv If your project uses many data sets, potentially from many different data providers one could easily add sub-folders for each data provider: ./ |src/ |- data/ |- data-provider-a/ |- dataset1.csv |- dataset2.csv |- data-provider-b/ |- dataset3.txt |- dataset4.txt i.e. add a further layer of sub-folders. This might add extra clarity. !! EXERCISE !! Think about whether you could might want additional sub-folders in any of the other src directories. Provide some examples you could use, and explain the pros and cons of using them. 2.4 Project Structure III: Separating Input Parameters from Code When we explored the sub-folders of src we found three folders that ended with -specs: data-specs, model-specs, and table-specs. These folders are used to store alternate specifications that we want to use as part of our data analysis, or table construction. To get a better understanding of why these folders exist in our project structure, we are going to take a look at their contents. Let’s look at what files are located inside the data-specs folder: $ ls src/data-specs/ We see: subset_intermediate.json subset_nonoil.json subset_oecd.json The file names are somewhat meaningful on their own - they appear to be some way of subsetting data (selecting some rows). If we look inside one of these files: cat src/data-specs/subset_oecd.json Which returns: { &quot;KEEP_CONDITION&quot;: &quot;oecd == 1&quot; } We see what could be interpreted as a variable KEEP_CONDITION which stores a string \"oecd == 1\". !! EXERCISE !! Inspect the each of the JSON files in src/model-specs. Explain what you have found, and how you could use these files as part of the project. (HINT:) what does src/analyis/estimate_ols_model.R do? This propensity to pre-maturely throw out code and data organization is something the authors of this tutorial can relate to, as it has often tempted us in our own research - and even when writing this tutorial. ↩︎ Our own personal experience suggests that this side benefit reaps just as many rewards. Knowing immediately where to look for pieces of code and parameterizations because all our projects look identical in their organization is definitely a frustration reducing benefit of structure.↩︎ We discussed downloading these files HERE along with any software you need to install to work through subsequant chapters. ↩︎ LINK BACK TO HOW TO DO THIS ACROSS OSes.↩︎ "],["initial-steps-with-snakemake.html", "Chapter 3 Initial Steps with Snakemake 3.1 Snakefile – Writing Our First Rule 3.2 The Snakemake Command – Executing a Rule 3.3 Reading and Writing Files – First Steps in a Data Science Pipeline 3.4 Redundant Executions – (Don’t) Run a Rule Repeatedly", " Chapter 3 Initial Steps with Snakemake Learning goals Where and how do we write Snakemake rules? How do we execute a rule? How can rules interact with input and output files? How does Snakemake deal with redundant executions? Now that we know the structure of our project, we are ready to start building a fully reproducible pipeline. Snakemake expects instructions in a file called Snakefile in the current folder. A Snakefile is a collection of rules that together define the order in which a project will be executed. We have added an empty Snakefile in the main project folder. You can edit this file in a text editor of your choice. In the remainder of this tutorial we will fill the file together with a pipeline which reproduces the results from Mankiw, Romer, and Weil (1992). 3.1 Snakefile – Writing Our First Rule By law any software tutorial is required to start with a hello world example, so we’ll follow court and do the same. To do so, we open Snakefile in a text editor of our choice and write the following text into the file: rule hello_world: shell: &#39;echo &quot;Hello World!&quot;&#39; Snakemake instructions are written in Python 3 with a few extra objects and methods provided by Snakemake. The structure of our rule will look very familiar to anybody who has written Python functions before. This implies that indentation is interpreted by the language and that divinations from correct indentation will result in errors. The first line in our example defines the name of a rule, in this case hello_world. The part following shell: tells Snakemake that this rule executes a shell command, which in our case prints Hello World! to the console. 3.2 The Snakemake Command – Executing a Rule After saving our changes to Snakefile, we can execute the newly created rule by navigating to the project’s main folder and by typing Snakemake --cores 1. By default Snakemake will execute the first rule it encounters in a Snakefile. As our file only contains a single rule, it will execute the hello_world rule and print Hello World! to the console together with some additional information about the execution of the work flow. When a file contains more than a single rule, we often want to be more explicit and tell Snakemake to execute a particular rule like so: $ snakemake --cores 1 name_of_rule In our case snakemake --cores 1 hello_world will give the same output as the last execution. At this point you are probably correctly guessing that the long option --cores #n tell Snakemake how many CPU cores to use when executing a workflow. Running workflows in parallel can speed up complex workflows substantially. In this guide we will execute all workflows with a single core to make the code compatible with all computers but you are encouraged to choose a core count appropriate to your system instead. 3.3 Reading and Writing Files – First Steps in a Data Science Pipeline Like most research, our project starts with data management (also see Chapter XX). The data management subdirectory, src/data-management contains two scripts rename_variables.R gen_reg_vars.R which need to be executed sequentially. In this chapter, we want to run the script rename_variables.R to tidy up the variable names in our data set. In the next chapter, we will then add gen_reg_vars.R, which creates additional variables that will be needed to run some regressions in later steps, and learn how to execute both rules sequentially to in a first, minimum pipeline. 3.3.1 Using the Rscript command to execute R scripts We can run any R script with the Rscript shell command. The command expects the path to an .R script as the first argument. Simple R scripts that do not require additional options can be just run as such: Rscript PATH/TO/SCRIPT.R Most scripts in this project have been written to require additional parameters. We can run any R script that requires parameters with the --help option to see a list of arguments. Let us apply this and execute the rename_variables.R script with the --help flag: $ Rscript src/data-management/rename_variables.R --help This prints the following output to the console: Usage: src/data-management/rename_variables.R [options] Options: -d CHARACTER, --data=CHARACTER stata dataset file name -o CHARACTER, --out=CHARACTER output file name [default = out.csv] -h, --help Show this help message and exit This suggests the script expects a --data option which defines the filepath to a stata data set. This is the input dataset which the script transforms. a --out option which defines the filepath the transformed data set will be saved to. Next we will learn how to define inputs and outputs in snakemake. 3.3.2 The input and output arguments Our hello_world rule was very simple and did not interact with the file system. In a research workflow we typically want to interact with input files such as data or R scripts which produce one or more outputs like transformed data, tables, or graphs. A snakemake rule can be thought of as the recipe that combines the inputs to produce the outputs. Snakemake expects these components to be provided in a particular way so that it knows what to do with the information we provide. We are going to specify rules in the following format: rule rule_name: input: input_name1 = &quot;PATH/TO/input_one&quot;, input_name2 = &quot;PATH/TO/input_two&quot; output: output_name1 = &quot;PATH/TO/SAVE/output_one&quot;, output_name2 = &quot;PATH/TO/SAVE/output_two&quot; shell: &quot;HOW TO MIX IT ALL TOGETHER&quot; We can have as many inputs and outputs as the rule needs to function. Each input and each output are given names, for example input_name1 which take the value to the file path and name of the file. It is important to wrap each of these paths into quotations, and to separate each of the multiple inputs and outputs with a comma. 3.3.3 A rule to rename variables. Let us now define a snakemake rule which executes the rename_variables.R script with the correct input and output paths. To do so, we copy the example rule into our snakefile above the hello_world rule. Let us also rename the rule to rename_vars. The rule will use the rename_variables.R script as a first input. Let us therefore rename the first input into script and replace the right hand side with the path to the rename_variables.R script which we have used above when we printed the script’s help function. Our rename_vars rule should now look something like this: rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, input_name2 = &quot;PATH/TO/input_two&quot; output: output_name1 = &quot;PATH/TO/SAVE/output_one&quot;, output_name2 = &quot;PATH/TO/SAVE/output_two&quot; shell: &quot;HOW TO MIX IT ALL TOGETHER&quot; Next, we want to add any additional inputs and also specify any outputs that the file produces. From the help output we know the script expects a stata dataset as an additional input which we can find in the src/data/ folder. We rename the second input into data and replace the right hand side with the path to Mankiw, Romer, and Weil (1992)’s stata dataset. Finally, we replace the first output name with data and define an appropriate output file path as the right hand side. This is the location our transformed data will be saved to. After these steps, our rule should look something like this: rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;HOW TO MIX IT ALL TOGETHER&quot; Lastly, we need to edit the shell command to combine inputs and output. We know that we can execute the R script via the Rscript command with the path to the script as the first argument. Gladly, snakemake does not require it to write the full path out again. We can instead refer to the input’s and output’s which we have defined via their names which Snakemake expects within {}. Executing an R script can therefore be done via Rscript {input.script}. The dataset and the output location can be passed to our script as --data and --out arguments and we will pass it the input and output options which we have defined in our rule. Putting it all together will give us the following rule: rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; Note that \\ in the shell command allows us to write the command over several lines for better readability. They can be omitted when we write the command in a single line. After saving we can execute our newly rule via: $ snakemake --cores 1 rename_vars Snakemake will give us some console output informing us about the execution of our rule. A successful execution will be displayed through green text ending in TODO: Make green Finished job 0. 1 of 1 steps (100%) done If the execution was successful, we can look into our output directory to see if anything has happened: $ ls out/data/ which yields mrw_renamed.csv Our file has been created as we expected. 3.4 Redundant Executions – (Don’t) Run a Rule Repeatedly What would happen if the run the rule a second time after it has been successfully completed before? Let us execute the rename_vars rule a second time: $ snakemake --cores 1 rename_vars We see the following output: Building DAG of jobs... Nothing to be done. To understand the status of the project as seen by Snakemake, we can use the summary option as such: $ snakemake --summary In our case the output following output is printed to the console: Building DAG of jobs... output_file date rule version log-file(s) status plan out/data/mrw_renamed.csv Thu Jan 10 20:31:08 2019 rename_vars - ok no update Snakemake tells us the status of the output file of the rename_vars rule is ok and that it plans not to update the file. Now, let us what happens if we update one of the input files. The following command updates the time the rename_variables.R was last edited without changing the actual content of the file: $ touch src/data-management/rename_variables.R Running the summary command again $ snakemake --summary yields the following output: Building DAG of jobs... output_file date rule version log-file(s) status plan out/data/mrw_renamed.csv Thu Jan 10 20:31:08 2019 rename_vars - updated input files update pending Snakemake has detected that one input file is more recent than the output file. It therefore will execute the rule when asked to do so. Running the rule again via snakemake --cores 1 rename_vars will execute the rule. We see one mayor advantage of Snakemake over simpler and very common alternatives to execute projects. Often researchers chain do files together in a linear and static way. It is very common in such a workflow to comment out chunks of the code to prevent redundant executions to safe time. Snakemake keeps track of dependencies and updates to input files. We therefore do not have to worry about preventing the execution of previously run scripts to gain efficiency and safe time. We can simply run the most down stream rule of our project and let Snakemake figure out which upstream rules need to be re-run. We will explain later in this tutorial how to write an all rule which takes care of this. Containers "],["working-with-multiple-rules.html", "Chapter 4 Working With Multiple Rules 4.1 Creating a second rule 4.2 Clean Rules", " Chapter 4 Working With Multiple Rules Learning goals How do we chain Snakemake rules together? How does Snakemake know in which order to execute rules? Practice writing Snakemake rules What are clean rules and how can they make our life easier? In the last chapter we have learned how to write and execute a single Snakemake rule. This is obviously not sufficient for a real world research workflow which requires many different operations to be executed sequentially. in this chapter we will therefore learn how to work with several rules and how they interact with each other. 4.1 Creating a second rule The second step in our data management is to create additional variables we will use in our regression analysis. The script gen_reg_vars.R in the src/data-management folder does this for us. We are going to build a rule called gen_regression_vars to do this from Snakemake. Let’s see what arguments the script expects to be passed: $ Rscript src/data-management/gen_reg_vars.R --help Options: -d CHARACTER, --data=CHARACTER a csv file name -p NUMERIC, --param=NUMERIC Solow`s Constant, as a numeric [default = 0.05] -o CHARACTER, --out=CHARACTER output file name [default = out.csv] -h, --help Show this help message and exit We see that script expects three arguments: --data, the file path of the input file --out, the file path of the output file --param, Solow’s constant which defaults to 0.05, the parameter used in Mankiw, Romer, and Weil (1992). We can safely ignore this for now. To chain two rules together, the second one needs to use the output of the first rule as its input. This way the second script will receive all the changes the first script made so they are not lost. Let us incorporate this here and create a gen_regression_vars rule on top of the rename_vars rule. The rule takes two inputs: {input.script} will be gen_reg_vars.R and {input.data} will be the file defined as {output.data} in the rename_vars rule. Our new rule now should look something like this: rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot; Let us now add the filepath the output of the rule will be saved to. We will use a new filename in the out/data folder and choose mrw_complete.csv to indicate that this data set includes all data modifications the project requires: rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot;, output: data = &quot;out/data/mrw_complete.csv&quot; Finally, let us add the shell argument, which is identical to the one used in our first rule. Now, our rule is complete: rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot; output: data = &quot;out/data/mrw_complete.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; To find out how Snakemake perceives our projects, let’s use the summary option $ snakemake --summary which yields the following console output: Building DAG of jobs... output_file date rule version log-file(s) status plan out/data/mrw_complete.csv - gen_regression_vars - missing update pending out/data/mrw_renamed.csv Fri Jan 11 13:40:07 2019 rename_vars - ok no update Snakemake knows that the output of the rename_vars rule is up to date and therefore plans to only execute the gen_regression_vars rule as it’s output out/data/mrw_complete.csv does not yet exist. Let’s run snakemake to build our new file: $ snakemake --cores 1 When we look at our output directory $ ls out/data/ we see that the second output file has been created. mrw_complete.csv mrw_renamed.csv Exercise: Creating Rules Mankiw, Romer, and Weil (1992) estimate the Solow model for three subsets of data. The R script src/analysis/estimate_ols.R will estimate a OLS model for a given dataset when you provide the necessary inputs. The script expects an OLS specification. You can point to the basic OLS specification from Mankiw, Romer, and Weil (1992) in src/model-specs/model_solow.json. You will find the necessary data subsetting conditions in src/data-specs. Write one rule for each subset of the data. What inputs do you need to provide? What outputs will be produced? Write Snakemake rules to estimate the solow model for each subset of data. Solution What inputs do you need to provide? --data the file path of a csv dataset as an input. We use the output of our last rule: out/data/mrw_complete.csv. --model the file path of a regression model. We follow the hint and use src/model-specs/model_solow.json. --subset the file path of a sub setting condition. We will use one of the three files in src/data-specs for each rule respectively. --out the file path for the output file. What outputs will be produced? One .rds file for each ols model. This is a R data type appropriate for saving R objects. The rule therefore outputs one R object containing the regression results for each rule. Loading the .rds file in another script will allow us later to use the regression results to create regression tables and graphs. Write Snakemake rules to estimate the solow model for each subset of data. Incorporating everything will give us three rules similar to these: rule solow_intermediate: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_intermediate.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule solow_nonoil: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_nonoil.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_nonoil.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule solow_oecd: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_oecd.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_oecd.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; 4.2 Clean Rules We want to end this chapter with a little extra rule which will make our life easier down the road. So far we have built rules to move our replication of Mankiw, Romer, and Weil (1992) forward. As we continue to extend our Snakefile in the coming chapters we might want to be able to delete all of the produced outputs and make sure our projects builds cleanly from start to end. The manual way to do this is to go to our terminal window and enter the following command each time $ rm -rf out/* which deletes all contents of the out folder. To make this task a bit more comfortable, we write a clean rule which deletes all contents of the output folder. We can create a rule called clean at the bottom of our Snakefile that stores the shell command from above: rule clean: shell: &quot;rm -rf out/*&quot; Note that this rule has no inputs or outputs. To use this rule, we enter the following into our terminal: $ snakemake --cores 1 clean Notice that to use the clean rule we had to call the rule name, clean, explicitly. This is necessary as the rule is at the bottom of our Snakefile. Snakemake the first rule it finds at the top of the file when not explicitly told otherwise. Now if we look out the output of running the summary call with snakemake, we see the following output: snakemake --summary Building DAG of jobs... output_file date rule version log-file(s) status plan out/analysis/model_solow_subset_intermediate.rds - inter - missing update pending out/analysis/model_solow_subset_nonoil.rds - nonoil - missing update pending out/analysis/model_solow_subset_oecd.rds - oecd - missing update pending out/data/mrw_complete.csv - gen_regression_vars missing update pending out/data/mrw_renamed.csv - rename_vars missing update pending Which reveals snakemake’s plan the next time its run will be to build all outputs. Exercise: Creating Cleaning Rules So far we have written a cleaning rule that deletes everything in the out/ directory. Construct rules that would separately clean the out/data/ and out/analysis subdirectories. Why might we want to do this? Solution We only need to change the folders to delete in the shell command. A solution might look something like this: rule clean_data: shell: &quot;rm -rf out/data/*&quot; rule clean_analysis: shell: &quot;rm -rf out/analysis/*&quot; Splitting the clean rule gives us a bit more control on what we want to refresh. In practice, data cleaning and wrangling can be computationally very taxing. Especially later in a project’s life cycle these task seldomly change. It can therefore be beneficial and sufficient to only reset all other analysis via a clean rule that does not delete the transformed data. Containers "],["target-rules.html", "Chapter 5 Target Rules 5.1 Where we are now? 5.2 How Snakemake determines the build order when multiple rules are present 5.3 Dedicated target rules to execute multiple rules 5.4 Target rules can do more for us", " Chapter 5 Target Rules Learning Goals: How to build several parallel tasks at once 5.1 Where we are now? If you still have the hello_world rule in your Snakefile, now is a good moment to remove it. Then, your Snakefile should look something like this: ## Snakemake - MRW Replication ## ## @yourname # --- OLS Rules --- # rule solow_intermediate: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_intermediate.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule solow_nonoil: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_nonoil.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_nonoil.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule solow_oecd: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_oecd.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_oecd.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; # --- Data Management --- # rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot; output: data = &quot;out/data/mrw_complete.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; # --- Clean Rules --- # rule clean: shell: &quot;rm -rf out/*&quot; rule clean_data: shell: &quot;rm -rf out/data/*&quot; rule clean_analysis: shell: &quot;rm -rf out/analysis/*&quot; 5.2 How Snakemake determines the build order when multiple rules are present Now that we have worked with multiple rules and seen how one can execute another rule, let us try to understand the principle behind this. As we know, we can execute any rule in Snakefile explicitly by calling it by name: snakemake --cores 1 solow_intermediate When no rule name is explicitly given, Snakemake will execute the first rule it encounters at the top of Snakefile: snakemake --cores 1 If you followed the order of rules in the solution to the previous exercise, solow_intermediate is the first rule in Snakefile. In this case, both the explicit and implicit commands are equivalent. You can verify this by cleaning your project before executing either or by adding --summary to either command. The rule we ask Snakemake to execute either explicitly or implicitly is called a target rule. Snakemake focusses on executing this rule. When all necessary inputs to build the target rule exist, Snakemake will simply execute the rule and build the defined outputs. When Snakemake recognizes that a necessary input to execute the target rule is missing, Snakemake will try to build it through the other rules present in Snakefile. To illustrate this, let us stick to our present project and assume the target rule is solow_intermediate. The following graph shows all rules which lead up to the target rule: mrw_complete.csv is a necessary input to solow_intermediate. If the file does not exist, Snakemake will search through Snakefile for another rule which has mrw_complete.csv as its output – in our case gen_regression_vars. In this case, Snakemake will first execute gen_regression_vars before executing the target rule solow_intermediate. If necessary inputs are missing to execute gen_regression_vars, Snakemake will search for other rules to produce it and so forth… To allow Snakemake to work properly, the input and output relationships in a project need to follow a directed acyclic graph (DAG). TODO: Finish sentence. Directed here means that…. Acyclic implies that there can be no two rules which require output from one another as inputs, either directly or in a larger loop. This has a few implications on how we should define our intermediate input and output files. We should follow the following best practices to prevent problems: No rule should have the same file as input and output. It is often common practice to load a certain dataset, perform operations on it and overwrite the original file. While there are good reasons to never do this in any workflow, this behavior can lead to more severe problems when using Snakemake. Snakemake only searches for rules which can create input files which do not exist. If a rule overwrites an already existing input file, Snakemake would not recognize it as a dependency and simply ignore it. This implies that a project might run through cleanly and produce incorrect results without this being detected. In our workflow, we prevented such behavior by making sure the gen_regression_vars rule writes its results to a new file. No two rules should have the same output. When Snakemake searches for a file to create a missing input file, it will execute the first rule it encounters which produces this file. To make sure our input and output relationships are explicit and reproducible, we do not want the order of the rules in a Snakefile determine which rule is actually executed. A project should have a clear direction. A project should have a clear trajectory. In practice this typically starts with data manipulations which are necessary to perform analysis, which in turn will be used to create plots and tables, which finally end up in a paper or set of slides. In such a typical workflow, directedness implies for example that a regression table ist not an input in the data manipulation steps. A project cannot be circular. Any circularity would let Snakemake search for rules in an infinite loop. 5.3 Dedicated target rules to execute multiple rules Currently, our project has three rules which perform OLS regressions parallel to one another at the top of Snakefile. As only one rule can be the target, it would require us to execute Snakemake three times – once for every OLS regression: snakemake --cores 1 solow_intermediate snakemake --cores 1 solow_nonoil snakemake --cores 1 solow_oecd While either command would simplify our lives a bit by also running all the necessary data manipulations automatically, it’s still a bit silly to run Snakemake repeatedly. After all it is our goal to make our workflow reproducible via a single line of code. Now that we understand the concept of target rules, we will try to make use of them to our advantage. As we know, Snakemake will execute any rule which can produce an output which a target rule requires as its input. Adding a new dedicated target rule which requires the outputs of our three solow models will allow us to execute all three rules via a single line of code. We add this rule at the top of Snakefile and name it solow_target. The rule only has three inputs, one for the output of each solow model like so: rule solow_target: input: intermediate = &quot;out/analysis/model_solow_subset_intermediate.rds&quot;, nonoil = &quot;out/analysis/model_solow_subset_nonoil.rds&quot;, oecd = &quot;out/analysis/model_solow_subset_oecd.rds&quot; After saving Snakefile, we can inspect how Snakemake perceives the state of our project with the summary option snakemake --summary output_file date rule version log-file(s) status plan out/analysis/model_solow_subset_intermediate.rds Thu Feb 18 14:18:20 2021 solow_intermediate - ok no update out/analysis/model_solow_subset_nonoil.rds - - - - missing update pending out/analysis/model_solow_subset_oecd.rds - - - - missing update pending out/data/mrw_complete.csv Thu Feb 11 16:29:24 2021 gen_regression_vars - ok no update out/data/mrw_renamed.csv Tue Feb 9 15:46:38 2021 rename_vars - ok no update What does this mean? Snakemake sees the three model_solow_* outputs as files it needs to create for the solow_target rule. Out of these three files, the first, model_solow_subset_intermediate.rds already exists and is up to date after our last run above. The other two files, model_solow_subset_nonoil.rds and model_solow_subset_oecd.rds, do not yet exist and must be created. The rule column presents the rule Snakemake found to create the respective output file. The output therefore shows, that a single run of solow_target would create all files in our project. In the following exercise, you will practice executing the project with help of our new target rule. Exercise: Using target rules It is time to put our new and shiny target rule to work and execute our full workflow with a single line of code. Delete the content of the out/ folder with help of the clean rule. Verify that the output folder is empty. Execute the solow_target rule and build all outputs in a single swoop. Solution Delete the content of the out/ folder with help of the clean rule. snakemake --cores 1 clean Verify that the output folder is empty. ls out Nothing to show here This shows that the output folder is indeed cleaned properly. Execute the solow_target rule and build all outputs in a single swoop. snakemake --cores 1 solow_target Building DAG of jobs... Using shell: /bin/bash Provided cores: 1 (use --cores to define parallelism) Rules claiming more threads will be scaled down. Job counts: count jobs 1 gen_regression_vars 1 rename_vars 1 solow_intermediate 1 solow_nonoil 1 solow_oecd 1 solow_target 6 [...] [Thu Feb 18 14:38:11 2021] localrule solow_target: input: out/analysis/model_solow_subset_intermediate.rds, out/analysis/model_solow_subset_nonoil.rds, out/analysis/model_solow_subset_oecd.rds jobid: 0 [Thu Feb 18 14:38:11 2021] Finished job 0. 6 of 6 steps (100%) done The output which Snakemake prints to the screen starts with the plan Snakemake develops to generate the input files of solow_target. We see that it needs to run each of the other rules once. Snakemake then plans to execute solow_target at the end. The execution of the whole chain of rules sums up to six different task that Snakemake will execute. The middle part of the output, which we ommit here as [...] contains the messages which would be printed to screen for each of the rules which are executed. This contains information about Snakemake’s execution as well as the console output that R would print if we execute each of the R scripts in an appropriate IDE such as Rstudio. Finally the last part prints Snakemake’s reports about the final solow_target rule. In accordance with the rule itself, it only features input files. The bottom message contains information about the successful execution of all six rules with a completion rate of 100%. Snakemake is done. 5.4 Target rules can do more for us As we know, the last execution of our target rule did nothing substantial, as the rule only includes the input part of the rule. In practice we can also use the target rule to perform other small tasks for us with the desired output. When compiling LaTeX files, it is often easier to have all latex inputs in a separate folder. LaTeX is typically not very good with relatives paths and likes to create many temporary files which we probably do not want to keep. To keep things tidy, we can therefore include the cleaning of unwanted LaTeX temporary files and copy the output PDF to the main directory of our project for convenience. Some of us also use the target rule to copy the output PDF to a shared folder where colleagues can access them. When you share your work in a Dropbox folder, it can be a large time saver to not copy the output PDF there manually after each update of the draft. "],["wildcards.html", "Chapter 6 Wildcards 6.1 The starting point of our Snakefile 6.2 Using wildcards to tidy up our code", " Chapter 6 Wildcards Learning Goals: How to minimize repetition in the code Adding a target rule to execute all regressions at the same time is good progress to simplify the execution of our project. In this chapter we want to make our life easier to write and maintain our project. If we look at the solow_ rules we see that there is quite a lot of replication of code between them. All three rules use the same script, data, and model inputs. They only differ in the data subsetting found under {input.subset} and the output file define under {output.estimate}. Ideally, we want Snakefile to feature the minimum amount of duplication possible. This has a few advantages: Limiting redundancy allows us to make changes to the code only once rather than multiple times (imagine changing the {data.input} file). Any additional copy of code introduces extra opportunities of typos or other errors and makes the build file generally harder to read and navigate. And if we’re completely honest, it’s also a just a bit ugly… While maintaining duplicate code for three rules is still bearable, redundancy becomes a larger problem when we have many duplicate rule executions. Imagine managing 15 OLS specifications instead of three. In this chapter we will learn how to unify such redundancies and collapse the three rules solow_nonoil, solow_oecd, and solow_intermediate into a single rule. To do so, we create a new variable, iSubset, that can iterate through the three .json files that contain the subset filters. In Snakemake, these variables are called wildcards. 6.1 The starting point of our Snakefile The top of our Snakefile at the moment looks something like this: # --- Build Rules --- # rule solow_target: input: intermediate = &quot;out/analysis/model_solow_subset_intermediate.rds&quot;, nonoil = &quot;out/analysis/model_solow_subset_nonoil.rds&quot;, oecd = &quot;out/analysis/model_solow_subset_oecd.rds&quot; rule solow_intermediate: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_intermediate.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule solow_nonoil: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_nonoil.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_nonoil.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule solow_oecd: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_oecd.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_oecd.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; [...] where [...] stands for the bottom part of our Snakefile that we will not modify in this chapter. 6.2 Using wildcards to tidy up our code Let us now replace the three solow_ rules by a single rule which we call solow_model where the unique parts of the {input.subset} and {output.estimate} are replaced with our wildcard variable {iSubset}. The relevant part now looks something like this: rule solow_target: input: intermediate = &quot;out/analysis/model_solow_subset_intermediate.rds&quot;, nonoil = &quot;out/analysis/model_solow_subset_nonoil.rds&quot;, oecd = &quot;out/analysis/model_solow_subset_oecd.rds&quot; rule solow_model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/subset_{iSubset}.json&quot; output: estimate = &quot;out/analysis/model_solow_subset_{iSubset}.rds&quot;, shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; [...] We notice right away how much more concise the relevant part of our script has become. With a bit of practice it is also much easier to read and understand what these rules do. We wrapped our wildcard iSubset in curly parentheses so that Snakemake knows that this part is a variable which we want to substitute with the name of one of the subsets. This is conceptually similar to what we have done in our shell commands. Let us clean our output folder again via snakemake --cores 1 clean and try to execute solow_model as a target rule: $ snakemake --cores 1 solow_model Building DAG of jobs... WorkflowError: Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards. What has happened? Snakemake will not execute a rule that contains wildcards because it does not know what values to substitute into iSubset. Gladly, we already have a target rule which does not contain wildcards and explicitly specifies the input files we want to create, our trusty run_solow rule. Let us therefore try to execute our target rule instead: $ snakemake --cores 1 solow_target Building DAG of jobs... Using shell: /bin/bash Provided cores: 4 Rules claiming more threads will be scaled down. Job counts: count jobs 1 gen_regression_vars 1 rename_vars 3 solow_model 1 solow_target 6 [...] [Thu Feb 18 19:50:04 2021] localrule solow_target: input: out/analysis/model_solow_subset_intermediate.rds, out/analysis/model_solow_subset_nonoil.rds, out/analysis/model_solow_subset_oecd.rds jobid: 0 [Thu Feb 18 19:50:04 2021] Finished job 0. 6 of 6 steps (100%) done Nice! Our error is gone and we have produced the same output as before an a much more manageable manner. We see one difference to our execution from the previous chapter in the printed job counts at the beginning. Instead of running all solow_* rules once, Snakemake sees that it needs to execute the solo_model rule instead three times — once for every subset. We might ask ourselves how the wildcard approach has worked. Essentially, Snakemake knows it needs to create the three outputs model_solow_subset_intermediate.rds, model_solow_subset_nonoil.rds, and model_solow_subset_oecd.rds\". To so so it looks for other rules which can produce these files. Snakemake finds the solow_model rule which can produce these files as its output when {iSubject} is replaced with intermediate, nonoil, and oecd respectively. However, for each execution of the rule it needs to fill {iSubject} with the same value in both the output and input part. Snakemake therefore then checks whether it can find the appropriate input files when filling {iSubject} with the corresponding value. When these files exist for the same wildcard value, it will take the corresponding input file to execute the rule. "],["automatic-expansion-of-wildcards.html", "Chapter 7 Automatic Expansion of Wildcards 7.1 Learning Goals 7.2 The expand() function 7.3 Expanding Multiple Wildcards", " Chapter 7 Automatic Expansion of Wildcards 7.1 Learning Goals TODO: Add Goals TODO: !!! State of the Snakefile !!! Add a box with how the snakefile needs to be at the beginning of this chapter 7.2 The expand() function When we look at our target rule: rule solow_target: input: intermediate = &quot;out/analysis/model_solow_subset_intermediate.rds&quot;, nonoil = &quot;out/analysis/model_solow_subset_nonoil.rds&quot;, oecd = &quot;out/analysis/model_solow_subset_oecd.rds&quot; We can see that each of inputs listed have similar structure: The outputs live on the same path out/analysis/, and The filenames of each output begin with model_solow and end with .rds. Only the name of the subset of data we are using changing. We can use the idea of wildcards from the previous chapter together with some additional Snakemake functionality to simplify this target rule. Using what we learned in Chapter XX, we can modify solow_target using a wildcard: rule solow_target: input: models = &quot;out/analysis/model_solow_{iSubset}.rds&quot;, If we try and run snakemake with our target rule written like this we see the following: $ snakemake --cores 1 solow_target We get an error: Building DAG of jobs... WildcardError in line 12 of /home/lachlan/teaching/snakemake-econ-r-learner/Snakefile: Wildcards in input files cannot be determined from output files: &#39;iSubset&#39; Recall that Snakemake won’t run a rule that contains a wildcard because it does not know how to resolve it, i.e. Snakemake doesn’t know to replace {iSubset} with either of subset_oecd, subset_intermediate or subset_nonoil. It is our job to tell Snakemake that this is what we want to happen. Luckily Snakemake has a functionality that allows us to tell it how to accept a wildcard and replace it with a set of specific values. The function we are looking for is the expand() function. As the name suggests its job is to expand a wildcard by replacing with with a specified value. All we need to do is tell Snakemake what to substitute in the wildcard’s place. We can use the expand() function as follows: rule solow_target: input: expand(&quot;out/analysis/model_solow_{iSubset}.rds&quot;, iSubset = DATA_SUBSET) This says replace the wildcard {iSubset} with some value DATA_SUBSET. Let’s look at the what this buys us. First, let’s clean up our output folder with snakemake clean so that we have an clean slate to work with: $ snakemake --cores 1 clean Now replace DATA_SUBSET in our target rule of the Snakefile with one of our data subsets the analysis uses, subset_nonoil: rule solow_target: input: expand(&quot;out/analysis/model_solow_{iSubset}.rds&quot;, iSubset = &quot;subset_nonoil&quot;) And let’s use the --summary option of snakemake to see what the build plan would be: $ snakemake --summary We see: output_file date rule version log-file(s) status plan out/analysis/model_solow_subset_nonoil.rds - solow_model - missing update pending out/data/mrw_complete.csv - gen_regression_vars - missing update pending out/data/mrw_renamed.csv - rename_vars - missing update pending From the top line of the output, we can see that Snakemake wants to build the file out/analysis/model_solow_subset_nonoil.rds. This means Snakemake has indeed replaced the wildcard iSubset with subset_nonoil. And, because we were not thrown an error message, the problem of not being able to resolve a wildcard has been solved through the use of expand(). Now that we have the idea that expand() can replace a wilcard with a value, we want to know how to replace a wildcard with multiple values iteratively, i.e., first with subset_nonoil, then with subset_oecd and finally with subset_intermediate. We do this by passing a list, in the python sense of the word, of values that we want {iSubset} to take. For example, if we want to run the model on both subset_nonoil and subset_oecd: rule solow_target: input: expand(&quot;out/analysis/model_solow_{iSubset}.rds&quot;, iSubset = [&quot;subset_nonoil&quot;, &quot;subset_oecd&quot;]) If we then look at the summary: $ snakemake --summary We see: output_file date rule version log-file(s) status plan out/analysis/model_solow_subset_nonoil.rds - solow_model - missing update pending out/analysis/model_solow_subset_oecd.rds - solow_model - missing update pending out/data/mrw_complete.csv - gen_regression_vars - missing update pending out/data/mrw_renamed.csv - rename_vars - missing update pending The top two lines show the wildcard {iSubset} has indeed been replaced successively with subset_nonoil and then subset_oecd. The notion of constructing a list of values for Snakemake to iterate through is definitely beneficial, and progress has been made. However, specifying the list each time in the rule itself is not ideal for at least two reasons: If we need to use the same list multiple times across different rules we have to copy and paste it, and then be sure we update it correctly each time, and It can be hard to find the lists we are iterating over when we read through the Snakefile. We can easily overcome both limitations by creating our list of data subsets outside of the rule, and then referring the list by name as needed. To do this, at the top of our Snakefile we will create an area to store lists we will want to iterate through. For us right now, we want to create a list of data subsets, so let’s call the list DATA_SUBSET.8 Inside that list we put the three subsets: DATA_SUBSET = [ &quot;subset_oecd&quot;, &quot;subset_intermediate&quot;, &quot;subset_nonoil&quot; ] Then we inside our expand() we write iSubset = DATA_SUBSET so Snakemake knows to iterate through that list: rule solow_target: input: expand(&quot;out/analysis/model_solow_{iSubset}.rds&quot;, iSubset = DATA_SUBSET) Now when we look at the build plan with the summary option: $ snakemake --summary We see: output_file date rule version log-file(s) status plan out/analysis/model_solow_subset_oecd.rds - solow_model - missing update pending out/analysis/model_solow_subset_intermediate.rds - solow_model - missing update pending out/analysis/model_solow_subset_nonoil.rds - solow_model - missing update pending out/data/mrw_complete.csv - gen_regression_vars - missing update pending out/data/mrw_renamed.csv - rename_vars - missing update pending Which shows Snakemake wants to run the regression on all three data subsets. Let’s do that now: $ snakemake --cores 1 !! TO DO !! add the output here Exercise: Exploring the expand function I The MRW paper contains three plots. Each of these plots use the data subset of ‘intermediate’ countries. In the src/figures/ subdirectory, there are three scripts that reproduce the figures one script per plot.9 Write three rules fig_1, fig_2 and fig_3 to produce each figure. Each figure should be saved with the following name ‘out/figures/SCRIPTNAME.pdf’ Create a list called PLOTS that contains the file names of each of the three plot scripts (without .R). Use what you have learned about wildcards and the expand function to condense the three rules in (a) into one rule figures that can make each figure and a target rule make_figures so that snakemake --cores 1 figs_target builds all three figures, one after the other. HINT: The scripts are written in such a way that they accept exactly the same options. Using wildcards and the expand function extend the Snakefile to construct each figure. 7.2.1 SOLUTION: Use snakemake --cores 1 make_figures to run the figures # --- Dictionaries --- # DATA_SUBSET = [ &quot;subset_oecd&quot;, &quot;subset_intermediate&quot;, &quot;subset_nonoil&quot; ] PLOTS = [ &quot;aug_conditional_convergence&quot;, &quot;conditional_convergence&quot;, &quot;unconditional_convergence&quot; ] # --- Build Rules --- # rule solow_target: input: expand(&quot;out/analysis/model_solow_{iSubset}.rds&quot;, iSubset = DATA_SUBSET) rule solow_model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/model_solow_{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule make_figures: input: expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = PLOTS) rule figure: input: script = &quot;src/figures/{iFigure}.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: fig = &quot;out/figures/{iFigure}.pdf&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --subset {input.subset} \\ --out {output.fig}&quot; rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot;, param = &quot;src/data-specs/param_solow.json&quot; output: data = &quot;out/data/mrw_complete.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --param {input.param} \\ --out {output.data}&quot; rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; rule clean: shell: &quot;rm -rf out/*&quot; rule clean_data: shell: &quot;rm -rf out/data/*&quot; rule clean_analysis: shell: &quot;rm -rf out/analysis/*&quot; Exercise: Exploring the expand() function II So far we have estimated the basic Solow model. MRW also estimate an augmented version of the Solow model, adding human capital. The regression model required to estimate the augmented model is written up in src/model-specs/model_aug_solow.json. Write the pair of rules aug_solow_model and aug_solow_target to estimate this model on each of the data subsets in DATA_SUBSET HINT: Use the expand() function together with the estimate_ols.R script to estimate the augmented solow model on each of the three data subsets. The rule structures should look very similar to what we have done so far. 7.2.2 Solution # --- Dictionaries --- # DATA_SUBSET = [ &quot;subset_oecd&quot;, &quot;subset_intermediate&quot;, &quot;subset_nonoil&quot; ] PLOTS = [ &quot;aug_conditional_convergence&quot;, &quot;conditional_convergence&quot;, &quot;unconditional_convergence&quot; ] # --- Build Rules --- # rule solow_target: input: expand(&quot;out/analysis/model_solow_{iSubset}.rds&quot;, iSubset = DATA_SUBSET) rule aug_solow_target: input: expand(&quot;out/analysis/model_aug_solow_{iSubset}.rds&quot;, iSubset = DATA_SUBSET) rule solow_model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/model_solow_{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule aug_solow_model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_aug_solow.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/model_aug_solow_{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule make_figures: input: expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = PLOTS) rule figure: input: script = &quot;src/figures/{iFigure}.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: fig = &quot;out/figures/{iFigure}.pdf&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --subset {input.subset} \\ --out {output.fig}&quot; rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot;, param = &quot;src/data-specs/param_solow.json&quot; output: data = &quot;out/data/mrw_complete.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --param {input.param} \\ --out {output.data}&quot; rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; rule clean: shell: &quot;rm -rf out/*&quot; rule clean_data: shell: &quot;rm -rf out/data/*&quot; rule clean_analysis: shell: &quot;rm -rf out/analysis/*&quot; 7.3 Expanding Multiple Wildcards The rules used to estimate the standard Solow model, and the augmented Solow model have very similar structure: rule solow_target: input: expand(&quot;out/analysis/model_solow_{iSubset}.rds&quot;, iSubset = DATA_SUBSET) rule aug_solow_target: input: expand(&quot;out/analysis/model_aug_solow_{iSubset}.rds&quot;, iSubset = DATA_SUBSET) rule solow_model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_solow.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/model_solow_{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule aug_solow_model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/model_aug_solow.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/model_aug_solow_{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; We see that the only difference between the aug_solow_target and solow_target rules is that the filename differs by three letters: out/analysis/model_aug_solow_{iSubset}.rds versus out/analysis/model_solow_{iSubset}.rds. And, when we look at aug_solow_model and compare it to solow_model we again see only small differences: the model changes from model_aug_solow.json to model_solow.json and the outputs change in a similar way. We want to exploit this similarity and condense these four rules into two: one that estimates the model, named model and a target rule that will run each specification, names run_models. We are going to do this by creating a new list, called MODELS. MODELS will contain two elements, “model_solow” and “model_aug_solow”. Let’s add this to the top of our Snakefile: # --- Dictionaries --- # MODELS = [ &quot;model_solow&quot;, &quot;model_aug_solow&quot; ] DATA_SUBSET = [ &quot;subset_oecd&quot;, &quot;subset_intermediate&quot;, &quot;subset_nonoil&quot; ] PLOTS = [ &quot;aug_conditional_convergence&quot;, &quot;conditional_convergence&quot;, &quot;unconditional_convergence&quot; ] We are now going to delete the target rules aug_solow_target and solow_target, and the estimation rules aug_solow_model and solow_model. Let’s replace them with the following: rule run_models: input: expand(&quot;out/analysis/{iModel}_{iSubset}.rds&quot;, iSubset = DATA_SUBSET, iModel = MODELS) rule model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/{iModel}.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/{iModel}_{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; The rule model is a generalization of the solow_model we had before. We have added an extra wildcard, {iModel} so that the rule can estimate a wider set of models that we specify in the src/model-specs folder. It’s input.model now takes a wildcard {iModel} and it’s output is a {iModel} - {iSubset} pair. Similarly, the target rule run_models is a generalization of solow_target. Instead of taking only the model_solow models as inputs - and looking for how to build those, it now takes any {iModel} as an input. Notice there are now two wildcards in the expand() function, {iSubset} as before, and now also {iModel}. We then naturally have to tell Snakemake which lists to use to expand each wildcard, DATA_SUBSET for {iSubset}, and MODELS for {iModel}. Let’s clean our output folder and then use Snakemakes --summary to see what Snakemake wants to build. We anticipate it will want to to the data cleaning steps, and then six regression models. The six regression models will be three each of model_solow and model_aug_solow. Each model should be run on each data subset. $ snakemake clean --cores 1 $ snakemake --summary Here’s the output we get: MissingInputException in line 42 of /home/lachlan/teaching/snakemake-econ-r-learner/Snakefile: Missing input files for rule model: src/data-specs/oecd.json src/model-specs/model_solow_subset.json What is going on here? The problem is as follows: because our filenames have underscores _ in them and we are using the underscore to separate {iModel} and {iSubset} Snakemake cannot figure out where the first wildcard substitution ends. We think model_solow should be substituted as {iModel} and subset_oecd should be substituted for iSubset. Unfortunately Snakemake can’t handle use of underscores in both file names and wildcard separation simultaneously. Worry not, there is simple ways around this. We are going to solve it by separating the wildcards with a period . rather than an underscore.10 We make this change in both the model and run_models rules. This breaks any ambiguity about where a wildcard ends and another begins. Our updated rules are: rule run_models: input: expand(&quot;out/analysis/{iModel}.{iSubset}.rds&quot;, iSubset = DATA_SUBSET, iModel = MODELS) rule model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/{iModel}.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/{iModel}.{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; Now we again try to get the summary information from Snakemake: $ snakemake --summary There’s no longer an error, and instead we see: output_file date rule version log-file(s) status plan out/analysis/model_solow.subset_oecd.rds - - - - missing update pending out/analysis/model_aug_solow.subset_oecd.rds - - - - missing update pending out/analysis/model_solow.subset_intermediate.rds - - - - missing update pending out/analysis/model_aug_solow.subset_intermediate.rds - - - - missing update pending out/analysis/model_solow.subset_nonoil.rds - - - - missing update pending out/analysis/model_aug_solow.subset_nonoil.rds - - - - missing update pending out/data/mrw_complete.csv - gen_regression_vars - missing update pending out/data/mrw_renamed.csv - rename_vars - missing update pending which confirms that Snakemake wants to do our data cleaning steps before estimating the six regressions. Let’s do it: $ snakemake --cores 1 Our Snakemake rule graph - which shows the rules we execute in our workflow is now: TODO? Some explanation? And the DAG is: The DAG makes crystal clear what our workflow is doing: Snakemake wants to build the inputs listed in run_models It can build those inputs by running the model rule six times when it iterates over the wildcards To run each model, it needs the clean data, so it needs to run the two data cleaning steps first !!! REMARK !!! Notice how the figures aren’t produced automatically. Do you remember why? If not, revisit Chapter XX. You can build them with snakemake --cores 1 make_figures The capitalization of the list DATA_SUBSET is not essential. We do it to separate lists that we will iterate through from other parts of our Snakefile. This means whenever we see a capitalized name, we know it is a list that we want to iterate through.↩︎ This is not entirely true, we are yet to figure out how to get the y-axis range from the original paper.↩︎ There’s other ways to go here. You could continue to use underscores but add a few letters inbetween the wildcards and it would also work. This means out/analysis/{iModel}.{iSubset}.rds would become out/analysis/{iModel}_SOMETEXT_{iSubset}.rds. It’s equally valid.↩︎ "],["automating-list-construction-for-wildcard-expansion.html", "Chapter 8 Automating List Construction for Wildcard Expansion 8.1 Learning Objectives 8.2 State of the Snakefile 8.3 The glob_wildcards Function 8.4 Looking at the DAG (redux)", " Chapter 8 Automating List Construction for Wildcard Expansion 8.1 Learning Objectives TODO! 8.2 State of the Snakefile TODO: Make this a box + code fold To proceed with this chapter, we expect you Snakefile to look as follows: # --- Dictionaries --- # MODELS = [ &quot;model_solow&quot;, &quot;model_aug_solow&quot; ] DATA_SUBSET = [ &quot;subset_oecd&quot;, &quot;subset_intermediate&quot;, &quot;subset_nonoil&quot; ] PLOTS = [ &quot;aug_conditional_convergence&quot;, &quot;conditional_convergence&quot;, &quot;unconditional_convergence&quot; ] # --- Build Rules --- # rule run_models: input: expand(&quot;out/analysis/{iModel}.{iSubset}.rds&quot;, iSubset = DATA_SUBSET, iModel = MODELS) rule model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/{iModel}.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/{iModel}.{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule make_figures: input: expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = PLOTS) rule figure: input: script = &quot;src/figures/{iFigure}.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: fig = &quot;out/figures/{iFigure}.pdf&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --subset {input.subset} \\ --out {output.fig}&quot; rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot;, param = &quot;src/data-specs/param_solow.json&quot; output: data = &quot;out/data/mrw_complete.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --param {input.param} \\ --out {output.data}&quot; rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; rule clean: shell: &quot;rm -rf out/*&quot; rule clean_data: shell: &quot;rm -rf out/data/*&quot; rule clean_analysis: shell: &quot;rm -rf out/analysis/*&quot; In the previous chapter when we wanted to expand a wildcard we manually specified the valued we wanted them to take by constructing a list. By doing so, the beginning of our Snakefile looks like this: MODELS = [ &quot;model_solow&quot;, &quot;model_aug_solow&quot; ] DATA_SUBSET = [ &quot;subset_oecd&quot;, &quot;subset_intermediate&quot;, &quot;subset_nonoil&quot; ] FIGURES = [ &quot;aug_conditional_convergence&quot;, &quot;conditional_convergence&quot;, &quot;unconditional_convergence&quot; ] This is not too problematic when we only have a few values that we want the wildcard to take, but manually specifying long lists can get tedious and is prone to error. Snakemake has a built in function, glob_wildcards that will help us to remove the manual listing of values that we had to do so far. 8.3 The glob_wildcards Function Let’s start by trying to replace the MODELS list that we manually specified with a more automated approach. The glob_wildcards() function takes one input - the path of the files that we want to search combined with the part of the file name we want to extract written as a wildcard. It then looks to find anything that along the specidied path, collecting what it finds along the way. Let’s look at glob_wildcards in action to get a sense of what will happen. Replace our manually specified MODELS list with the following code: MODELS = glob_wildcards(&quot;src/model-specs/{fname}&quot;) This says look inside the src/model-specs/ folder, and return any files we find there. {fname} is the name of the wildcard we use to tell snakemake we want the filenames from inside the folder. The name fname itself doesn’t matter, we could use whatever we liked, but we decided {fname} is suggestive we want filenames so it helps us to understand what the code is doing. Next, we want to look at what is now stored in MODELS. This is a litle tricker than it should be, but if we add a print(MODELS) command in our Snakefile we can get the value printed to screen. Let’s do that, and then do a dry run. MODELS = glob_wildcards(&quot;src/model-specs/{fname}&quot;) print(MODELS) then: $ snakemake --dryrun What we are interested in is the first printed lines (in white text): Wildcards(fname=[&#39;.gitkeep&#39;, &#39;model_solow.json&#39;, &#39;model_aug_cc_restr.json&#39;, &#39;model_solow_restr.json&#39;, &#39;model_cc.json&#39;, &#39;model_ucc.json&#39;, &#39;model_aug_solow_restr.json&#39;, &#39;model_aug_cc.json&#39;, &#39;model_aug_solow.json&#39;]) Here we see that all files are returned. Compared to our original MODELS list we see three differences There are more .json files, because there is potentially more specifications to run There is a .gitkeep file Each ‘fname’ ends with .json, which we did’t have earlier. is not a problem, it reflects that there is more potential analysis files that we haven’t manually specified. But (2) and (3) are problematic. We can remove the .gitkeep file and the .json file endings from the list with one step: telling the glob_wildcards() function to only return the part of the filename that comes before the .json. We do this by updating our glob_wildcards() call as follows; MODELS = glob_wildcards(&quot;src/model-specs/{fname}.json&quot;) print(MODELS) This says store in the list MODELS the part of the filename that comes before .json for all files in src/model-specs. The .gitkeep will also not be not returned, because this file does not have a .json ending: Now let’s try the dry run again: $ snakemake --dryrun Now the first line is: Wildcards(fname=[&#39;model_solow&#39;, &#39;model_aug_cc_restr&#39;, &#39;model_solow_restr&#39;, &#39;model_cc&#39;, &#39;model_ucc&#39;, &#39;model_aug_solow_restr&#39;, &#39;model_aug_cc&#39;, &#39;model_aug_solow&#39;]) That’s definitely an improvement. We note that the dry-run command is still giving us an error though, complaining about missing input files. That is because currently MODELS is not yet the the list that is was when we manually specified it.11 The list we want is inside the Wildcards() class, and we can see it has the name fname which is the name of the wildcard we assigned ourselves. Our final step is to extract this list so that we can use it like our old MODELS list. We do this as follows: MODELS = glob_wildcards(&quot;src/model-specs/{fname}.json&quot;).fname print(MODELS) Read this as “get the list called fname from inside the Wildcards() object &amp; assign it to the name MODELS”. Now if we do a dry-run: $ snakemake --dryrun We are returned the following lines at the beginning of the output: [&#39;model_solow&#39;, &#39;model_aug_cc_restr&#39;, &#39;model_solow_restr&#39;, &#39;model_cc&#39;, &#39;model_ucc&#39;, &#39;model_aug_solow_restr&#39;, &#39;model_aug_cc&#39;, &#39;model_aug_solow&#39;] ... which looks like the list we had manually entered, but with more elements. The dry-run output also showed us that there are many steps it wants to run next time it is executed. This is because it will want to obtain the output from each of 8 models estimated on all three of the data subsets in DATA_SUBSET. Recall that in Chapter XX we estimated two models, aug_solow and solow. This means there are six models left to run, each across 3 data sets, for a total of 18 steps. If we look through the dry run output we see that is indeed what is going to happen: Building DAG of jobs... Job counts: count jobs 18 model 1 run_models 19 [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_oecd.json output: out/analysis/model_aug_solow_restr.subset_oecd.rds jobid: 8 wildcards: iModel=model_aug_solow_restr, iSubset=subset_oecd [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_nonoil.json output: out/analysis/model_solow_restr.subset_nonoil.rds jobid: 17 wildcards: iModel=model_solow_restr, iSubset=subset_nonoil [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_intermediate.json output: out/analysis/model_aug_cc_restr.subset_intermediate.rds jobid: 12 wildcards: iModel=model_aug_cc_restr, iSubset=subset_intermediate [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_nonoil.json output: out/analysis/model_aug_solow_restr.subset_nonoil.rds jobid: 24 wildcards: iModel=model_aug_solow_restr, iSubset=subset_nonoil [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_oecd.json output: out/analysis/model_ucc.subset_oecd.rds jobid: 7 wildcards: iModel=model_ucc, iSubset=subset_oecd [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_intermediate.json output: out/analysis/model_aug_cc.subset_intermediate.rds jobid: 10 wildcards: iModel=model_aug_cc, iSubset=subset_intermediate [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_nonoil.json output: out/analysis/model_aug_cc.subset_nonoil.rds jobid: 18 wildcards: iModel=model_aug_cc, iSubset=subset_nonoil [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_nonoil.json output: out/analysis/model_aug_cc_restr.subset_nonoil.rds jobid: 20 wildcards: iModel=model_aug_cc_restr, iSubset=subset_nonoil [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_oecd.json output: out/analysis/model_cc.subset_oecd.rds jobid: 3 wildcards: iModel=model_cc, iSubset=subset_oecd [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_intermediate.json output: out/analysis/model_solow_restr.subset_intermediate.rds jobid: 9 wildcards: iModel=model_solow_restr, iSubset=subset_intermediate [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_oecd.json output: out/analysis/model_aug_cc.subset_oecd.rds jobid: 2 wildcards: iModel=model_aug_cc, iSubset=subset_oecd [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_intermediate.json output: out/analysis/model_ucc.subset_intermediate.rds jobid: 15 wildcards: iModel=model_ucc, iSubset=subset_intermediate [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_oecd.json output: out/analysis/model_aug_cc_restr.subset_oecd.rds jobid: 4 wildcards: iModel=model_aug_cc_restr, iSubset=subset_oecd [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_nonoil.json output: out/analysis/model_cc.subset_nonoil.rds jobid: 19 wildcards: iModel=model_cc, iSubset=subset_nonoil [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_intermediate.json output: out/analysis/model_aug_solow_restr.subset_intermediate.rds jobid: 16 wildcards: iModel=model_aug_solow_restr, iSubset=subset_intermediate [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_oecd.json output: out/analysis/model_solow_restr.subset_oecd.rds jobid: 1 wildcards: iModel=model_solow_restr, iSubset=subset_oecd [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_intermediate.json output: out/analysis/model_cc.subset_intermediate.rds jobid: 11 wildcards: iModel=model_cc, iSubset=subset_intermediate [Fri Feb 19 21:46:25 2021] rule model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_nonoil.json output: out/analysis/model_ucc.subset_nonoil.rds jobid: 23 wildcards: iModel=model_ucc, iSubset=subset_nonoil [Fri Feb 19 21:46:25 2021] localrule run_models: input: out/analysis/model_solow_restr.subset_oecd.rds, out/analysis/model_aug_cc.subset_oecd.rds, out/analysis/model_cc.subset_oecd.rds, out/analysis/model_aug_cc_restr.subset_oecd.rds, out/analysis/model_aug_solow.subset_oecd.rds, out/analysis/model_solow.subset_oecd.rds, out/analysis/model_ucc.subset_oecd.rds, out/analysis/model_aug_solow_restr.subset_oecd.rds, out/analysis/model_solow_restr.subset_intermediate.rds, out/analysis/model_aug_cc.subset_intermediate.rds, out/analysis/model_cc.subset_intermediate.rds, out/analysis/model_aug_cc_restr.subset_intermediate.rds, out/analysis/model_aug_solow.subset_intermediate.rds, out/analysis/model_solow.subset_intermediate.rds, out/analysis/model_ucc.subset_intermediate.rds, out/analysis/model_aug_solow_restr.subset_intermediate.rds, out/analysis/model_solow_restr.subset_nonoil.rds, out/analysis/model_aug_cc.subset_nonoil.rds, out/analysis/model_cc.subset_nonoil.rds, out/analysis/model_aug_cc_restr.subset_nonoil.rds, out/analysis/model_aug_solow.subset_nonoil.rds, out/analysis/model_solow.subset_nonoil.rds, out/analysis/model_ucc.subset_nonoil.rds, out/analysis/model_aug_solow_restr.subset_nonoil.rds jobid: 0 Job counts: count jobs 18 model 1 run_models 19 This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. Let’s run Snakemake to get all of the estimates. $ snakemake The power of using glob_wildcards() is clear. It’s a relatively easy way to construct a list to iterate over as part of a wildcard expansion based on the names of files. 8.4 Looking at the DAG (redux) With the addition of all of these new model estimates, Figure XX shows that the project’s DAG has expanded in complexity. The DAG again makes clear what our workflow is doing. Reading from the bottom of the figure upwards: Snakemake wants to build the inputs listed in run_models. This is the pairwise combination of all MODELS that were found in src/model-specs and all DATA_SUBSETS found in src/data-specs It can build those inputs by running the model rule \\(8 \\times 3 = 24\\) times when it iterates over the wildcards To run each model, it needs the clean data, so it needs to run the two data cleaning steps first !!! REMARK !!! Notice (again) how the figures aren’t produced automatically. Do you remember why? If not, revisit Chapter XX. You can build them with snakemake --cores 1 make_figures. Chapter YY will show us how to get all the outputs build with one run of Snakemake. Exercise: Exploring glob_wildcards() The Snakemake file still has two manually specified lists, DATA_SUBSET and PLOTS. Use the glob_wildcards() function to automate the the list construction. Clean the output directory Run Snakemake so that all models and all figures have been produced. 8.4.1 Solution Will need to run Snakemake twice snakemake --cores 1 run_models and snakemake --cores 1 make_figures. # --- Dictionaries --- # MODELS = glob_wildcards(&quot;src/model-specs/{fname}.json&quot;).fname DATA_SUBSET = glob_wildcards(&quot;src/data-specs/{fname}.json&quot;).fname PLOTS = glob_wildcards(&quot;src/figures/{fname}.R&quot;).fname # --- Build Rules --- # rule run_models: input: expand(&quot;out/analysis/{iModel}.{iSubset}.rds&quot;, iSubset = DATA_SUBSET, iModel = MODELS) rule model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/{iModel}.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/{iModel}.{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; rule make_figures: input: expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = PLOTS) rule figure: input: script = &quot;src/figures/{iFigure}.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: fig = &quot;out/figures/{iFigure}.pdf&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --subset {input.subset} \\ --out {output.fig}&quot; rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot;, param = &quot;src/data-specs/param_solow.json&quot; output: data = &quot;out/data/mrw_complete.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --param {input.param} \\ --out {output.data}&quot; rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; rule clean: shell: &quot;rm -rf out/*&quot; rule clean_data: shell: &quot;rm -rf out/data/*&quot; rule clean_analysis: shell: &quot;rm -rf out/analysis/*&quot; If one replaces print(MODELS) with print(type(MODELS)) we learn that MODELS is currently a class object rather than a list. It remains unclear to us as authors why this is what the Snakemake designers made this decision, but we have learned to embrace it. We hope you do too.↩︎ "],["make-tables.html", "Chapter 9 Make Tables 9.1 The State of the Snakefile 9.2 Building a single table 9.3 Building all tables at once", " Chapter 9 Make Tables Now that we have learned how to make our code more concise, we want to move forward and apply the new concepts from the previous part to finish our replication. In this chapter we will create a rule which generates all the regression tables in Mankiw, Romer, and Weil (1992). 9.1 The State of the Snakefile MODELS = glob_wildcards(&quot;src/model-specs/{fname}.json&quot;).fname # note this filter is only needed coz we are running an older version of the src files, will be updated soon DATA_SUBSET = glob_wildcards(&quot;src/data-specs/{fname}.json&quot;).fname DATA_SUBSET = list(filter(lambda x: x.startswith(&quot;subset&quot;), DATA_SUBSET)) PLOTS = glob_wildcards(&quot;src/figures/{fname}.R&quot;).fname ############################################## # OLS TARGET ############################################## rule run_models: input: expand(&quot;out/analysis/{iModel}.{iSubset}.rds&quot;, iSubset = DATA_SUBSET, iModel = MODELS) ############################################## # OLS RULES ############################################## rule model: input: script = &quot;src/analysis/estimate_ols_model.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, model = &quot;src/model-specs/{iModel}.json&quot;, subset = &quot;src/data-specs/{iSubset}.json&quot; output: estimate = &quot;out/analysis/{iModel}.{iSubset}.rds&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.estimate}&quot; ############################################## # figs TARGET ############################################## rule make_figures: input: expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = PLOTS) ############################################## # MAKING FIGS ############################################## rule figure: input: script = &quot;src/figures/{iFigure}.R&quot;, data = &quot;out/data/mrw_complete.csv&quot;, subset = &quot;src/data-specs/subset_intermediate.json&quot; output: fig = &quot;out/figures/{iFigure}.pdf&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --subset {input.subset} \\ --out {output.fig}&quot; ############################################## # DATA MANAGEMENT ############################################## rule gen_regression_vars: input: script = &quot;src/data-management/gen_reg_vars.R&quot;, data = &quot;out/data/mrw_renamed.csv&quot;, param = &quot;src/data-specs/param_solow.json&quot; output: data = &quot;out/data/mrw_complete.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --param {input.param} \\ --out {output.data}&quot; rule rename_vars: input: script = &quot;src/data-management/rename_variables.R&quot;, data = &quot;src/data/mrw.dta&quot; output: data = &quot;out/data/mrw_renamed.csv&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data}&quot; ############################################## # CLEANING RULES ############################################## rule clean: shell: &quot;rm -rf out/*&quot; rule clean_data: shell: &quot;rm -rf out/data/*&quot; rule clean_analysis: shell: &quot;rm -rf out/analysis/*&quot; 9.2 Building a single table Let us first start by making a single, explicit rule to create our first table. You can find the R script which does this under src/tables/regression_table.R. As always, we start by using the help function to learn the necessary input parameters the script expects: Rscript src/table/regression_table.R --help Usage: src/tables/regression_table.R [options] Options: -s CHARACTER, --spec=CHARACTER a json dictionary of table parameters -o CHARACTER, --out=CHARACTER output file name [default = out.tex] -h, --help Show this help message and exit Our model expects two arguments: --spec, a json dictionary with table parameters. This includes the models this tables contains as well as table specifications such as headers, column names etc. --out, the output filename the LaTeX code will be saved to We will make the first table with table_01.json as the spec argument and an appropriate output filename: rule table: input: script = &quot;src/tables/regression_table.R&quot;, spec = &quot;src/table-specs/table_01.json&quot; output: table = &quot;out/tables/table_01.tex&quot; shell: &quot;Rscript {input.script} \\ --spec {input.spec} \\ --out {output.table}&quot; If all model files have been created in the last chapter, then this rule should run through with a single executed task via snakemake --cores 1 table Additional to the usual Snakemake output, the LaTeX code of the table is printed to the screen. Let us check that the table has been successfully created in the out/tables folder: ls out/tables table_01.tex Nice, let us now clean our output folder and try to execute the table rule again. snakemake --cores 1 clean snakemake --cores 1 table [...] [Sat Feb 20 21:19:20 2021] rule table: input: src/tables/regression_table.R, src/table-specs/table_01.json output: out/tables/table_01.tex jobid: 0 [...] In gzfile(file, &quot;rb&quot;) : cannot open compressed file &#39;/Users/ubergm/Documents/projects/snakemake/snakemake-econ-r-learner/out/analysis/model_solow.subset_nonoil.rds&#39;, probable reason &#39;No such file or directory&#39; Execution stopped [Sat Feb 20 21:19:20 2021] Error in rule table: jobid: 0 output: out/tables/table_01.tex shell: Rscript src/tables/regression_table.R --spec src/table-specs/table_01.json --out out/tables/table_01.tex (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!) Shutting down, this might take some time. Exiting because a job execution failed. Look above for error message Complete log: /Users/ubergm/Documents/projects/snakemake/ snakemake-econ-r-learner/.snakemake/log/2021-02-20T211920.004018.snakemake.log We got an error. Looking at the R output, we see that the file out/analysis/model_solow.subset_nonoil.rds could not be found. Why is this? The model specification includes the path to the necessary models featured in table 1. Therefore R is looking for them in the output folder of the model rules. However, we have not included the model outputs as necessary inputs to our table rule. Snakemake therefore does not check if they exist and will not build them either. This is easy to fix. We just need to include the model inputs from the run_models target rule from the previous chapter into our new rule. This tells Snakemake to create all models before creating the regression table. Copying this part and adding it as {input.models} makes our table rule a target for the model outputs. We should end up with a rule like this: rule table: input: script = &quot;src/tables/regression_table.R&quot;, spec = &quot;src/table-specs/table_01.json&quot;, models = expand(&quot;out/analysis/{iModel}.{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) output: table = &quot;out/tables/table_01.tex&quot; shell: &quot;Rscript {input.script} \\ --spec {input.spec} \\ --out {output.table}&quot; Let us clean and rerun the rule: snakemake --cores 1 clean snakemake --cores 1 table Checking the output folder again, we see that all models ls out/analysis model_aug_cc.subset_intermediate.rds model_aug_cc_restr.subset_nonoil.rds model_aug_solow.subset_oecd.rds model_cc.subset_intermediate.rds model_solow.subset_nonoil.rds model_solow_restr.subset_oecd.rds model_aug_cc.subset_nonoil.rds model_aug_cc_restr.subset_oecd.rds model_aug_solow_restr.subset_intermediate.rds model_cc.subset_nonoil.rds model_solow.subset_oecd.rds model_ucc.subset_intermediate.rds model_aug_cc.subset_oecd.rds model_aug_solow.subset_intermediate.rds model_aug_solow_restr.subset_nonoil.rds model_cc.subset_oecd.rds model_solow_restr.subset_intermediate.rds model_ucc.subset_nonoil.rds model_aug_cc_restr.subset_intermediate.rds model_aug_solow.subset_nonoil.rds model_aug_solow_restr.subset_oecd.rds model_solow.subset_intermediate.rds model_solow_restr.subset_nonoil.rds model_ucc.subset_oecd.rds and the table ls out/tables table_01.tex have been successfully created. 9.3 Building all tables at once Being able to build a single table and all the models is of course nice but we want to build all six tables from Mankiw, Romer, and Weil (1992) with a single line of code. To do so elegantly, we will employ all tools we have learned in the last part. You will practice this in the following exercise. Exercise We want to generalize our table building script in this exercise to automatically create all six tables from Mankiw, Romer, and Weil (1992). Follow these steps to accomplish this: Add a wildcard to the table rule which can take the varying part of the {input.spec} and {output.table} parts. To do so, replace table_01 with {iTable} in both. Create a new target rule called make_tables. The rule expands the output argument of the table rule. It uses the {iTable} wildcard from a list called TABLES. Create the TABLES list using the glob_wildcards function which matches filenames without the file ending from the src/table-spec folder. Clean your output folder. Run the make_tables rule. Verify that all six tables have been created in the output folder. Solution Putting Steps 1-3 together should give us these three new parts: TABLES = glob_wildcards(&quot;src/table-specs/{fname}.json&quot;).fname rule make_tables: input: expand(&quot;out/tables/{iTable}.tex&quot;, iTable = TABLES) rule table: input: script = &quot;src/tables/regression_table.R&quot;, spec = &quot;src/table-specs/{iTable}.json&quot;, models = expand(&quot;out/analysis/{iModel}.{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET), output: table = &quot;out/tables/{iTable}.tex&quot; shell: &quot;Rscript {input.script} \\ --spec {input.spec} \\ --out {output.table}&quot; [...] Clean your output folder. snakemake --cores 1 clean Run the make_tables rule. snakemake --cores 1 make_tables Verify that all six tables have been created in the output folder. ls out/tables The expected output are the six table files: table_01.tex table_02.tex table_03.tex table_04.tex table_05.tex table_06.tex Containers "],["building-all-outputs-at-once.html", "Chapter 10 Building all outputs at once 10.1 Creating an all rule", " Chapter 10 Building all outputs at once In our analysis pipeline that we have constructed so far, we need to make two calls to snakemake to run all of our analysis: snakemake estimate_models - to estimate OLS regressions snakemake make_figs - to construct the figures Whilst this is not too time consuming, it would be desirable to only have to run Snakemake once and have all our analysis in (1) and (2) be executed. That is our next goal. So that we are all starting from the same directory structure, let’s clean our output directory, removing all estimated models and figures: $ snakemake clean Now if we list the contents of the out directory and any potential subdirectories: $ ls -R out/ we see that there are no contents remaining: out/: 10.1 Creating an all rule Out goal is to combine all the outputs from the estimate_models and make_figs rules. Let’s take a look at the structure of each of these rules so that we get a sense of what we might need to do., Here is the estimate_models rule: rule estimate_models: input: expand(&quot;out/analysis/{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) And the make_figs rule: rule make_figs: input: expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = FIGURES) These rules have a common structure: both having only an input. From other rules that we have written, we know that a rule can have multiple inputs if we name them. This suggests a way forward. We are going to create a new rule called all that contains two inputs, combining the inputs from the separate rules:12: figs, which contains the inputs from our make_figs rule models, which contains the inputs from our estimate_models rule As ever, we will have to be mindful that if we have multiple inputs, have the trailing commas located wherever one input is followed by another. Let’s put this rule as the first in our Snakefile: # --- Build Rules --- # rule all: input: figs = expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = FIGURES), models = expand(&quot;out/analysis/{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) Let’s check if this all rule does as desired - building all outputs. If we run snakemake --dryrun all, we get the following output: Building DAG of jobs... Job counts: count jobs 1 all 3 figures 1 gen_regression_vars 24 ols_model 1 rename_vars 30 [Tue Feb 5 16:23:01 2019] rule rename_vars: input: src/data-management/rename_variables.R, src/data/mrw.dta output: out/data/mrw_renamed.csv jobid: 29 [Tue Feb 5 16:23:01 2019] rule gen_regression_vars: input: src/data-management/gen_reg_vars.R, out/data/mrw_renamed.csv, src/data-specs/param_solow.json output: out/data/mrw_complete.csv jobid: 28 [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_oecd.json output: out/analysis/model_aug_cc_ols_subset_oecd.rds jobid: 1 wildcards: iModel=model_aug_cc, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/data-specs/subset_oecd.json output: out/analysis/model_solow_ols_subset_oecd.rds jobid: 2 wildcards: iModel=model_solow, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_oecd.json output: out/analysis/model_solow_restr_ols_subset_oecd.rds jobid: 4 wildcards: iModel=model_solow_restr, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_oecd.json output: out/analysis/model_cc_ols_subset_oecd.rds jobid: 5 wildcards: iModel=model_cc, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_intermediate.json output: out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds jobid: 27 wildcards: iModel=model_aug_cc_restr, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow.json, src/data-specs/subset_intermediate.json output: out/analysis/model_aug_solow_ols_subset_intermediate.rds jobid: 9 wildcards: iModel=model_aug_solow, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow.json, src/data-specs/subset_nonoil.json output: out/analysis/model_aug_solow_ols_subset_nonoil.rds jobid: 13 wildcards: iModel=model_aug_solow, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_nonoil.json output: out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds jobid: 15 wildcards: iModel=model_aug_cc_restr, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule figures: input: src/figures/aug_conditional_convergence.R, out/data/mrw_complete.csv, src/data-specs/subset_intermediate.json output: out/figures/aug_conditional_convergence.pdf jobid: 18 wildcards: iFigure=aug_conditional_convergence [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_oecd.json output: out/analysis/model_ucc_ols_subset_oecd.rds jobid: 25 wildcards: iModel=model_ucc, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_oecd.json output: out/analysis/model_aug_solow_restr_ols_subset_oecd.rds jobid: 26 wildcards: iModel=model_aug_solow_restr, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc_restr.json, src/data-specs/subset_oecd.json output: out/analysis/model_aug_cc_restr_ols_subset_oecd.rds jobid: 14 wildcards: iModel=model_aug_cc_restr, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow.json, src/data-specs/subset_oecd.json output: out/analysis/model_aug_solow_ols_subset_oecd.rds jobid: 3 wildcards: iModel=model_aug_solow, iSubset=subset_oecd [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/data-specs/subset_intermediate.json output: out/analysis/model_solow_ols_subset_intermediate.rds jobid: 7 wildcards: iModel=model_solow, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_nonoil.json output: out/analysis/model_cc_ols_subset_nonoil.rds jobid: 23 wildcards: iModel=model_cc, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_intermediate.json output: out/analysis/model_solow_restr_ols_subset_intermediate.rds jobid: 10 wildcards: iModel=model_solow_restr, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_intermediate.json output: out/analysis/model_aug_cc_ols_subset_intermediate.rds jobid: 11 wildcards: iModel=model_aug_cc, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] rule figures: input: src/figures/unconditional_convergence.R, out/data/mrw_complete.csv, src/data-specs/subset_intermediate.json output: out/figures/unconditional_convergence.pdf jobid: 24 wildcards: iFigure=unconditional_convergence [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_cc.json, src/data-specs/subset_intermediate.json output: out/analysis/model_cc_ols_subset_intermediate.rds jobid: 8 wildcards: iModel=model_cc, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow_restr.json, src/data-specs/subset_nonoil.json output: out/analysis/model_solow_restr_ols_subset_nonoil.rds jobid: 12 wildcards: iModel=model_solow_restr, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_solow.json, src/data-specs/subset_nonoil.json output: out/analysis/model_solow_ols_subset_nonoil.rds jobid: 16 wildcards: iModel=model_solow, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_cc.json, src/data-specs/subset_nonoil.json output: out/analysis/model_aug_cc_ols_subset_nonoil.rds jobid: 17 wildcards: iModel=model_aug_cc, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule figures: input: src/figures/conditional_convergence.R, out/data/mrw_complete.csv, src/data-specs/subset_intermediate.json output: out/figures/conditional_convergence.pdf jobid: 19 wildcards: iFigure=conditional_convergence [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_intermediate.json output: out/analysis/model_ucc_ols_subset_intermediate.rds jobid: 20 wildcards: iModel=model_ucc, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_nonoil.json output: out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds jobid: 21 wildcards: iModel=model_aug_solow_restr, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_ucc.json, src/data-specs/subset_nonoil.json output: out/analysis/model_ucc_ols_subset_nonoil.rds jobid: 22 wildcards: iModel=model_ucc, iSubset=subset_nonoil [Tue Feb 5 16:23:01 2019] rule ols_model: input: src/analysis/estimate_ols_model.R, out/data/mrw_complete.csv, src/model-specs/model_aug_solow_restr.json, src/data-specs/subset_intermediate.json output: out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds jobid: 6 wildcards: iModel=model_aug_solow_restr, iSubset=subset_intermediate [Tue Feb 5 16:23:01 2019] localrule all: input: out/figures/conditional_convergence.pdf, out/figures/unconditional_convergence.pdf, out/figures/aug_conditional_convergence.pdf, out/analysis/model_solow_ols_subset_oecd.rds, out/analysis/model_aug_cc_restr_ols_subset_oecd.rds, out/analysis/model_solow_restr_ols_subset_oecd.rds, out/analysis/model_cc_ols_subset_oecd.rds, out/analysis/model_ucc_ols_subset_oecd.rds, out/analysis/model_aug_solow_restr_ols_subset_oecd.rds, out/analysis/model_aug_cc_ols_subset_oecd.rds, out/analysis/model_aug_solow_ols_subset_oecd.rds, out/analysis/model_solow_ols_subset_nonoil.rds, out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds, out/analysis/model_solow_restr_ols_subset_nonoil.rds, out/analysis/model_cc_ols_subset_nonoil.rds, out/analysis/model_ucc_ols_subset_nonoil.rds, out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds, out/analysis/model_aug_cc_ols_subset_nonoil.rds, out/analysis/model_aug_solow_ols_subset_nonoil.rds, out/analysis/model_solow_ols_subset_intermediate.rds, out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds, out/analysis/model_solow_restr_ols_subset_intermediate.rds, out/analysis/model_cc_ols_subset_intermediate.rds, out/analysis/model_ucc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds, out/analysis/model_aug_cc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_ols_subset_intermediate.rds jobid: 0 Job counts: count jobs 1 all 3 figures 1 gen_regression_vars 24 ols_model 1 rename_vars 30 The output shows that we have achieved our desired goal. The all rule builds all outputs. Notice that if you examine the order that Snakemake plans to execute the jobs that: First the data management steps are executed to prepare data This is what we would expect, as we cannot do any analysis without cleaned data The order in which the figures and regressions are executed are mixed together This is because the figures and regression outputs do not depend on each other Now we build our entire project13: $ snakemake all The name all is a convention, we could name it anything we please↩︎ Notice that we placed the all rule as the first rule in our Snakefile. Snakemake’s default behaviour is to build the first rule in the file, so we could instead type only snakemake and get the same result.↩︎ "],["adding-parameters.html", "Chapter 11 Adding Parameters 11.1 Motivating Example: Constructing a Regression Table from OLS Results 11.2 Creating a Rule with params", " Chapter 11 Adding Parameters In our analysis pipeline so far we have always passed files to our R scripts. While we promote this practice, and wrapping important information into .json files, sometimes it seems like overkill to write a new json file to contain one line of configuration. An alternative to passing these json files is to use Snakemake’s built in params arguments, which are rule specific, to store information that we want to pass to our R Script. This goal of this chapter is to show how to use params to pass across a piece of information. 11.1 Motivating Example: Constructing a Regression Table from OLS Results So far, we have estimated a series of OLS regressions and stored there output inside the out/analysis directory. Typically once we have estimated one or more models, we want to format the output into a regression table that we can insert into a written document like a paper or set of presentation slides. In the folder src/tables/ we can see that there are a series of R scripts: $ ls src/tables/ which prints to the screen: tab01_textbook_solow.R tab03_ucc_solow.R tab05_cc_aug_solow.R tab02_augment_solow.R tab04_cc_solow.R tab06_cc_aug_solow_restr.R This shows that the example is designed to build 6 tables. Each table has it’s own script that constructs it. We will start by constructing Table 1, from tab01_textbook_solow.R Let’s have a look at what information this script expects us to pass using the help flag: Rscript src/tables/tab01_textbook_solow.R --help Usage: src/tables/tab01_textbook_solow.R [options] Options: -fp CHARACTER, --filepath=CHARACTER A directory path where models are saved -m CHARACTER, --models=CHARACTER A regex of the models to load -o CHARACTER, --out=CHARACTER output file name [default = out.tex] -h, --help Show this help message and exit From this we learn that we need to pass: --filepath, which is the directory where our OLS models are stored --models, a regular expression to tell R which models within the filepath to workwith --out, a .tex file where we want to direct the output Now we will work on constructing this rule. 11.2 Creating a Rule with params We are going to use the params option to pass across the filepath and the models regular expression into R. A sketch of the rule we want to create is: rule textbook_solow: input: script = , models = params: filepath = , model_expr = output: table = , shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table}&quot; There are two important points to notice about how we added params to our rules: params are added to the rule in a similar way to inputs and lists params are referenced identically to inputs and outputs in the shell command Now we need to decide what information needs to be entered into each line of our rule. EXPLAIN Our rule then becomes: rule textbook_solow: input: script = &quot;src/tables/tab01_textbook_solow.R&quot;, models = expand(&quot;out/analysis/{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET)), params: filepath = &quot;out/analysis/&quot;, model_expr = &quot;model_solow*.rds&quot; output: table = &quot;out/tables/tab01_textbook_solow.tex&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table}&quot; There are two ways to run this rule: Tell snakemake to run this rule explicitly, snakemake textbook_solow Because it is not the first rule of the Snakefile it isnt run by default Add the output of this rule to the all rule. Adds creating this table to our complete analysis pipeline We prefer (2). Hence we also update the all rule as follows: rule all: input: figs = expand(&quot;out/figures/{iFigure}.pdf&quot;, iFigure = FIGURES), models = expand(&quot;out/analysis/{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET), tab01 = &quot;out/tables/tab01_textbook_solow.tex&quot; If we then do a dry run to see what Snakemake plans to do14: Building DAG of jobs... Job counts: count jobs 1 all 1 textbook_solow 2 [Tue Feb 5 17:42:59 2019] rule textbook_solow: input: src/tables/tab01_textbook_solow.R, out/analysis/model_solow_ols_subset_oecd.rds, out/analysis/model_solow_ols_subset_nonoil.rds, out/analysis/model_solow_ols_subset_intermediate.rds, out/analysis/model_aug_cc_restr_ols_subset_oecd.rds, out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds, out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds, out/analysis/model_solow_restr_ols_subset_oecd.rds, out/analysis/model_solow_restr_ols_subset_nonoil.rds, out/analysis/model_solow_restr_ols_subset_intermediate.rds, out/analysis/model_cc_ols_subset_oecd.rds, out/analysis/model_cc_ols_subset_nonoil.rds, out/analysis/model_cc_ols_subset_intermediate.rds, out/analysis/model_ucc_ols_subset_oecd.rds, out/analysis/model_ucc_ols_subset_nonoil.rds, out/analysis/model_ucc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_restr_ols_subset_oecd.rds, out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds, out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds, out/analysis/model_aug_cc_ols_subset_oecd.rds, out/analysis/model_aug_cc_ols_subset_nonoil.rds, out/analysis/model_aug_cc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_ols_subset_oecd.rds, out/analysis/model_aug_solow_ols_subset_nonoil.rds, out/analysis/model_aug_solow_ols_subset_intermediate.rds output: out/tables/tab01_textbook_solow.tex jobid: 27 [Tue Feb 5 17:42:59 2019] localrule all: input: out/figures/conditional_convergence.pdf, out/figures/unconditional_convergence.pdf, out/figures/aug_conditional_convergence.pdf, out/analysis/model_solow_ols_subset_oecd.rds, out/analysis/model_solow_ols_subset_nonoil.rds, out/analysis/model_solow_ols_subset_intermediate.rds, out/analysis/model_aug_cc_restr_ols_subset_oecd.rds, out/analysis/model_aug_cc_restr_ols_subset_nonoil.rds, out/analysis/model_aug_cc_restr_ols_subset_intermediate.rds, out/analysis/model_solow_restr_ols_subset_oecd.rds, out/analysis/model_solow_restr_ols_subset_nonoil.rds, out/analysis/model_solow_restr_ols_subset_intermediate.rds, out/analysis/model_cc_ols_subset_oecd.rds, out/analysis/model_cc_ols_subset_nonoil.rds, out/analysis/model_cc_ols_subset_intermediate.rds, out/analysis/model_ucc_ols_subset_oecd.rds, out/analysis/model_ucc_ols_subset_nonoil.rds, out/analysis/model_ucc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_restr_ols_subset_oecd.rds, out/analysis/model_aug_solow_restr_ols_subset_nonoil.rds, out/analysis/model_aug_solow_restr_ols_subset_intermediate.rds, out/analysis/model_aug_cc_ols_subset_oecd.rds, out/analysis/model_aug_cc_ols_subset_nonoil.rds, out/analysis/model_aug_cc_ols_subset_intermediate.rds, out/analysis/model_aug_solow_ols_subset_oecd.rds, out/analysis/model_aug_solow_ols_subset_nonoil.rds, out/analysis/model_aug_solow_ols_subset_intermediate.rds, out/tables/tab01_textbook_solow.tex jobid: 0 Job counts: count jobs 1 all 1 textbook_solow 2 We see that snakemake only needs to create the table from our newly created rule. Now run snakemake to build the table: $ snakemake all and when finished if we list the contents of out/tables we see our new regression table has been created: $ ls out/tables/ tab01_textbook_solow.tex Exercise: Building Table 2 Using the same rule format as above, incorporate params into a new rule called augment_solow that constructs Table 2. An alternative would be to run snakemake --summary and examine the output.↩︎ "],["part-ii.html", "PART II", " PART II Some discussion before moving to the next chapter? "],["logging-output-and-errors.html", "Chapter 12 Logging Output and Errors 12.1 Logging Output 12.2 Logging Output and Errors in One File", " Chapter 12 Logging Output and Errors 12.1 Logging Output BLAH BLAH Let’s add a log file the textbook_solow rule. rule textbook_solow: input: script = &quot;src/tables/tab01_textbook_solow.R&quot;, models = expand(&quot;out/analysis/{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET), params: filepath = &quot;out/analysis/&quot;, model_expr = &quot;model_solow*.rds&quot; output: table = &quot;out/tables/tab01_textbook_solow.tex&quot; log: &quot;logs/tables/tab01_textbook_solow.Rout&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table} \\ &gt; {log}&quot; Notice .Rout is a convention …, can use anything we like. If we run Snakemake on the textbook_solow rule, it will tell us there is nothing to be done. We can force Snakemake to execute a rule by adding the --force flag: $ snakemake --force textbook_solow We see that as the R script is run the printout of the LaTeX table no longer goes to the screen, it has been redirected to the log file. This can be verified by viewing the contents of the log file: cat logs/tables/tab01_textbook_solow.Rout % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu % Date and time: Tue, Feb 05, 2019 - 06:34:51 PM \\begin{table}[!htbp] \\centering \\caption{Estimation of the Textbook Solow Model} \\label{} \\scriptsize \\begin{tabular}{@{\\extracolsep{5pt}}lcccccc} \\\\[-1.8ex]\\hline \\\\[-1.8ex] \\\\[-1.8ex] &amp; \\multicolumn{6}{c}{log(GDP per capita in 1965)} \\\\ &amp; Non-Oil &amp; Intermediate &amp; OECD &amp; Non-Oil &amp; Intermediate &amp; OECD \\\\ \\\\[-1.8ex] &amp; (1) &amp; (2) &amp; (3) &amp; (4) &amp; (5) &amp; (6)\\\\ \\hline \\\\[-1.8ex] log(I / GDP) &amp; 1.42$^{***}$ &amp; 1.32$^{***}$ &amp; 0.50 &amp; &amp; &amp; \\\\ &amp; (0.14) &amp; (0.17) &amp; (0.43) &amp; &amp; &amp; \\\\ log(n + g + $\\delta$) &amp; $-$1.99$^{***}$ &amp; $-$2.02$^{***}$ &amp; $-$0.74 &amp; &amp; &amp; \\\\ &amp; (0.56) &amp; (0.53) &amp; (0.85) &amp; &amp; &amp; \\\\ log(I / GDP) - log(n + g + $\\delta$) &amp; &amp; &amp; &amp; 1.49$^{***}$ &amp; 1.43$^{***}$ &amp; 0.55 \\\\ &amp; &amp; &amp; &amp; (0.12) &amp; (0.14) &amp; (0.37) \\\\ Constant &amp; 5.43$^{***}$ &amp; 5.35$^{***}$ &amp; 8.02$^{***}$ &amp; 6.87$^{***}$ &amp; 7.09$^{***}$ &amp; 8.62$^{***}$ \\\\ &amp; (1.58) &amp; (1.54) &amp; (2.52) &amp; (0.12) &amp; (0.15) &amp; (0.53) \\\\ \\hline \\\\[-1.8ex] Restricted Model &amp; No &amp; No &amp; No &amp; Yes &amp; Yes &amp; Yes \\\\ \\hline \\\\[-1.8ex] \\textit{N} &amp; 98 &amp; 75 &amp; 22 &amp; 98 &amp; 75 &amp; 22 \\\\ Adjusted R$^{2}$ &amp; 0.59 &amp; 0.59 &amp; 0.01 &amp; 0.59 &amp; 0.59 &amp; 0.06 \\\\ \\hline \\hline \\\\[-1.8ex] \\textit{Notes:} &amp; \\multicolumn{6}{r}{$^{***}$Significant at the 1 percent level.} \\\\ &amp; \\multicolumn{6}{r}{$^{**}$Significant at the 5 percent level.} \\\\ &amp; \\multicolumn{6}{r}{$^{*}$Significant at the 10 percent level.} \\\\ \\end{tabular} \\end{table} 12.2 Logging Output and Errors in One File Although this content has been redirected, there was still some information from the R session printed to screen. This was information about package loading. Why the separation? When we redirect using the &gt; what the bash shell is sending what’s called ‘stdout’ to the log file. stdout in this case is the table contents. The remaining information is called stderr, also known as standard error, is where BLAH.15 To integrate stderr into the same log file as the stdout we use &gt;&amp; instead of &gt;. Then our rule becomes: rule textbook_solow: input: script = &quot;src/tables/tab01_textbook_solow.R&quot;, models = expand(&quot;out/analysis/{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET), params: filepath = &quot;out/analysis/&quot;, model_expr = &quot;model_solow*.rds&quot; output: table = &quot;out/tables/tab01_textbook_solow.tex&quot; log: &quot;logs/tables/tab01_textbook_solow.Rout&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table} \\ &gt;&amp; {log}&quot; Now when we again run snakemake --force textbook_solow we see that no information from our R session is printed to the screen. Exercise: Logging all R Scripts Update each of the Snakemake rules to log all output to file. When finished, force the execution of the all rule, to build your entire project. HINT: Log files also accept wildcards {something}. Integrating wildcards into your log file names ensures each iteration of a rule gets a log file. We recommend the following conventions: Name the log file using the same name as the R script from which it stores the information. This makes it easier to find the log file later when debugging errors. For each subdirectory of the src folder, have a the same subdirectory in the log folder. Again this will facilitate finding the log file you want to inspect. In addition to the package loading information, if your code contains an error this is where that info will be printed.↩︎ "],["self-documenting-help.html", "Chapter 13 Self Documenting Help 13.1 Our Commenting Syle 13.2 Adding Further Comments 13.3 A help Rule", " Chapter 13 Self Documenting Help As we build up our Snakemake workflow the Snakefile becomes increasingly complex. It is easy to imagine that a future version of you, or a co-author, might have difficulty understanding what each rule is doing. This chapter introduces an approach to add some comments to the Snakefile to make it easier to navigate. We also add a new help rule so that we can get a summary of the comments printed to screen. 13.1 Our Commenting Syle If we look at the Snakefile in it’s current state, we see that there are a bare minimum amount of comments already integrated. Because Snakemake is a Python library all comments must begin with one #. We then added some additional formatting of comments to create two types.16 Our first type of comment is a ‘double hash’ - ##. We use this to represent information that someone who reads our code might find helpful to build understanding of the file. For example, the head of the Snakefile contains information on the name of the workflow and the authors: ## Snakemake - MRW Replication ## ## @yourname ## Second, we have broken up the Snakefile somewhat into sections such as Dictionaries, Build Rules and Clean Rules using the # --- Something --- # notation. 13.2 Adding Further Comments The information we have added in comments so far is not really enough to help us remember much. What we want is a simple one or two line summary of what each of our rules do so that we can look back at them in the future. We are going to make comments above each rule using the ## notation. The structure will follow a common pattern: rule_name : some description For example, for the textbook_solow rule: ## textbook_solow: construct a table of regression estimates for textbook solow model rule textbook_solow: input: script = &quot;src/tables/tab01_textbook_solow.R&quot;, models = expand(&quot;out/analysis/{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET)), params: filepath = &quot;out/analysis/&quot;, model_expr = &quot;model_solow*.rds&quot; output: table = &quot;out/tables/tab01_textbook_solow.tex&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table}&quot; Exercise: Adding Comments Add some comments using our suggested style to each of the Snakemake rules we have constructed so far. 13.3 A help Rule The Snakefile is now much easier to understand when reading through it. Future you will be grateful for the effort. We can go one step further: construct a rule that prints the output of these comments to the screen. Let’s call this rule help, so that if we run snakemake help all our useful comments will be printed to screen. We usually put this help rule at the bottom of our Snakefile, so that we don’t keep bumping into it as we work. The rule has the following structure: # --- Help Rules --- # ## help : prints help comments for Snakefile rule help: input: &quot;Snakefile&quot; shell: &quot;sed -n &#39;s/^##//p&#39; {input}&quot; The rule depends on the Snakefile itself - because if the file changes, the might too. The shell comand that we use is a little confusing when we look at it the first time. Intuitively heres what it is doing: The comments we want to print are those that have the double hash notation, ##. sed is a progam that can perform text manipulations to some input We want it to find all lines starting with ## in our input file … … and print out the remainder of those lines Let’s see it in action: $ snakemake help Building DAG of jobs... Using shell: /bin/bash Provided cores: 1 Rules claiming more threads will be scaled down. Job counts: count jobs 1 help 1 [Wed Feb 6 19:13:51 2019] rule help: input: Snakefile jobid: 0 Snakemake - MRW Replication @yourname all : builds all final outputs augment_solow : construct a table of estimates for augmented solow model textbook_solow : construct a table of regression estimates for textbook solow model make_figs : builds all figures figures : recipe for constructing a figure (cannot be called) estimate_models : estimates all regressions ols_models : recipe for estimating a single regression (cannot be called) gen_regression_vars: creates variables needed to estimate a regression rename_vars : creates meaningful variable names clean : removes all content from out/ directory help : prints help comments for Snakefile [Wed Feb 6 19:13:51 2019] Finished job 0. 1 of 1 steps (100%) done We see that after the usual Snakemake output telling us what it is doing, our comments are printed to screen. Exercise: Saving help output to file Modify the help rule so that the printed content is saved in a file called HELP.txt Do notice that the exact formatting of all this help is totally arbitrary. Noone tells you that two #’s or a mix of #’s and -’s are meaningful, correct or standard ways of putting comments in a file. Our choice reflects path dependence in the way we started writing Snakefiles, and we have found them useful and persisted with our initial choice. Feel free to create your own. The only important thing is the line starts with one # so that Snakemake doesnt try and execute that line as code.↩︎ "],["config-files.html", "Chapter 14 Config Files 14.1 The config.yaml filepath 14.2 Using paths from the config file", " Chapter 14 Config Files Motivation… 14.1 The config.yaml filepath In our project’s root directory we have the file config.yaml: $ ls -F config.yaml find_r_packages.sh HELP.txt install_r_packages.R logs/ out/ README.md REQUIREMENTS.txt sandbox/ Snakefile src/ If we look at the contents of config.yaml we see a collection of paths that match up to the paths in our project: $ cat config.yaml ROOT: &quot;.&quot; sub2root: &quot;../../&quot; src: &quot;src/&quot; log: &quot;logs/&quot; out: &quot;out/&quot; src_data: &quot;src/data/&quot; src_data_mgt: &quot;src/data-management/&quot; src_analysis: &quot;src/analysis/&quot; src_lib: &quot;src/lib/&quot; src_model_specs: &quot;src/model-specs/&quot; src_data_specs: &quot;src/data-specs/&quot; src_tables: &quot;src/tables/&quot; src_figures: &quot;src/figures/&quot; src_paper: &quot;src/paper/&quot; src_slides: &quot;src/slides/&quot; out_analysis: &quot;out/analysis/&quot; out_data: &quot;out/data/&quot; out_figures: &quot;out/figures/&quot; out_tables: &quot;out/tables/&quot; out_paper: &quot;out/paper/&quot; out_slides: &quot;out/slides/&quot; We can use this collection of paths to simplify the content of the Snakefile. First we must import the config.yaml file into the Snakefile. We import it using the configfile: notation, and so it’s easy for us to find, we place it at the very top of our Snakefile. ## Snakemake - MRW Replication ## ## @yourname ## # --- Importing Configuration Files --- # configfile: &quot;config.yaml&quot; # --- Dictionaries --- # &lt;...&gt; 14.2 Using paths from the config file We can now use the paths in config.yaml throughout the Snakefile. The text to the left of the colon in each line of the file is our reference to a particular path, so src_data will serve as our reference to the folder src/data/. Snakemake does not knows this path reference comes from the config file unless we instruct it to, so we reference it as config[\"src_data\"]. The path is then connected to the filename with a +. Hence we can reference the original MRW data located in src/data as config[\"src_data\"] + \"mrw.dta\" Here is the new version of the rule rename_vars using the config file to simplify the paths: ## rename_vars : creates meaningful variable names rule rename_vars: input: script = config[&quot;src_data_mgt&quot;] + &quot;rename_variables.R&quot;, data = config[&quot;src_data&quot;] + &quot;mrw.dta&quot; output: data = config[&quot;out_data&quot;] + &quot;mrw_renamed.csv&quot; log: config[&quot;out_log&quot;] + &quot;data_mgt/rename_variables.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data} \\ &gt;&amp; {log}&quot; Exercise: Using Config Files Go through the remainder of the Snakefile and replace all explicit paths with references from the config file. "],["subworkflows-divide-and-conquer.html", "Chapter 15 Subworkflows: Divide and Conquer 15.1 Subworkflow Basics 15.2 Subworkflow dependencies", " Chapter 15 Subworkflows: Divide and Conquer motivation 15.1 Subworkflow Basics Create a Subworkflow in data-management directory. Start with empty file: $ touch src/data-management/Snakefile Add the following info to src/data-management/Snakefile: # subworkflow - data-management # # @yourname # # --- Importing Configuration Files --- # configfile: &quot;config.yaml&quot; # --- Build Rules --- # ## gen_regression_vars: creates variables needed to estimate a regression rule gen_regression_vars: input: script = config[&quot;src_data_mgt&quot;] + &quot;gen_reg_vars.R&quot;, data = config[&quot;out_data&quot;] + &quot;out/data/mrw_renamed.csv&quot;, params = config[&quot;src_data_specs&quot;] + &quot;param_solow.json&quot;, output: data = config[&quot;out_data&quot;] + &quot;mrw_complete.csv&quot; log: config[&quot;log&quot;] + &quot;data-mgt/gen_reg_vars.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --param {input.params} \\ --out {output.data} \\ &gt;&amp; {log}&quot; ## rename_vars : creates meaningful variable names rule rename_vars: input: script = config[&quot;src_data_mgt&quot;] + &quot;rename_variables.R&quot;, data = config[&quot;src_data&quot;] + &quot;mrw.dta&quot; output: data = config[&quot;out_data&quot;] + &quot;mrw_renamed.csv&quot; log: config[&quot;log&quot;] + &quot;data-mgt/rename_variables.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --out {output.data} \\ &gt;&amp; {log}&quot; Can remove these rules from Snakefile in root directory. Snakefile in root needs to know there’s another Snakefile that creates outputs that rules in it depend on: # --- Dictionaries --- # &lt;...&gt; # --- Sub Workflows --- # # only need the final outputs here subworkflow data_mgt: workdir: config[&quot;ROOT&quot;] snakefile: config[&quot;src_data_mgt&quot;] + &quot;Snakefile&quot; # --- Build Rules --- # &lt;...&gt; And we need to tell it when an input in one of our rules is created from a subworkflow. Do this by, in this case, wrapping the output with data_mgt(output_name) Then the build rules section of our root directory Snakefile becomes: &lt;...&gt; # --- Build Rules --- # ## all : builds all final outputs rule all: input: figs = expand(config[&quot;out_figures&quot;] + &quot;{iFigure}.pdf&quot;, iFigure = FIGURES), models = expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET), tables = expand(config[&quot;out_tables&quot;] + &quot;{iTable}.tex&quot;, iTable = TABLES) ## augment_solow : construct a table of estimates for augmented solow model rule augment_solow: input: script = config[&quot;src_tables&quot;] + &quot;tab02_augment_solow.R&quot;, models = expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET), params: filepath = config[&quot;out_analysis&quot;], model_expr = &quot;model_aug_solow*.rds&quot; output: table = config[&quot;out_tables&quot;] + &quot;tab02_augment_solow.tex&quot;, log: config[&quot;log&quot;] + &quot;tables/tab02_augment_solow.Rout&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table} \\ &gt;&amp; {log}&quot; ## textbook_solow : construct a table of regression estimates for textbook solow model rule textbook_solow: input: script = config[&quot;src_tables&quot;] + &quot;tab01_textbook_solow.R&quot;, models = expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET), params: filepath = config[&quot;out_analysis&quot;], model_expr = &quot;model_solow*.rds&quot; output: table = config[&quot;out_tables&quot;] + &quot;tab01_textbook_solow.tex&quot; log: config[&quot;log&quot;] + &quot;tables/tab01_textbook_solow.Rout&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table} \\ &gt;&amp; {log}&quot; ## make_figs : builds all figures rule make_figs: input: expand(config[&quot;out_figures&quot;] + &quot;{iFigure}.pdf&quot;, iFigure = FIGURES) ## figures : recipe for constructing a figure (cannot be called) rule figures: input: script = config[&quot;src_figures&quot;] + &quot;{iFigure}.R&quot;, data = data_mgt(config[&quot;out_data&quot;] + &quot;mrw_complete.csv&quot;), subset = config[&quot;src_data_specs&quot;] + &quot;subset_intermediate.json&quot; output: fig = config[&quot;out_figures&quot;] + &quot;{iFigure}.pdf&quot; log: config[&quot;log&quot;]+ &quot;figures/{iFigure}.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --subset {input.subset} \\ --out {output.fig} \\ &gt;&amp; {log}&quot; ## estimate_models : estimates all regressions rule estimate_models: input: expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) ## ols_models : recipe for estimating a single regression (cannot be called) rule ols_model: input: script = config[&quot;src_analysis&quot;] + &quot;estimate_ols_model.R&quot;, data = data_mgt(config[&quot;out_data&quot;] + &quot;mrw_complete.csv&quot;), model = config[&quot;src_model_specs&quot;] + &quot;{iModel}.json&quot;, subset = config[&quot;src_data_specs&quot;] + &quot;{iSubset}.json&quot; output: model_est = config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, log: config[&quot;log&quot;] + &quot;analysis/{iModel}_ols_{iSubset}.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.model_est} \\ &gt;&amp; {log}&quot; &lt;...&gt; Let’s examine what happens with this new Workflow. Start with a clean output directory: $ snakemake clean Now do a dry run. $ snakemake --dryrun The beginning of the output looks like: Building DAG of jobs... Executing subworkflow data_mgt. Building DAG of jobs... Job counts: count jobs 1 gen_regression_vars 1 rename_vars 2 [Mon Feb 11 22:41:42 2019] rule rename_vars: input: src/data-management/rename_variables.R, src/data/mrw.dta output: out/data/mrw_renamed.csv log: logs/data-mgt/rename_variables.Rout jobid: 1 [Mon Feb 11 22:41:42 2019] rule gen_regression_vars: input: src/data-management/gen_reg_vars.R, out/data/mrw_renamed.csv, src/data-specs/param_solow.json output: out/data/mrw_complete.csv log: logs/data-mgt/gen_reg_vars.Rout jobid: 0 Job counts: count jobs 1 gen_regression_vars 1 rename_vars 2 Executing main workflow. Job counts: count jobs 1 all 1 augment_solow 3 figures 24 ols_model 1 textbook_solow 30 Explain what has happened here. 15.2 Subworkflow dependencies We want to go further…. So create a subworkflow for analysis too, containing all rules that estimate models. Create an empty Snakefile: $ touch src/analysis/Snakefile Move analysis rules across to src/analysis/Snakefile: # subworkflow - analysis # # @yourname # # --- Importing Configuration Files --- # configfile: &quot;config.yaml&quot; # --- Build Rules --- # ## estimate_models : estimates all regressions rule estimate_models: input: expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) ## ols_models : recipe for estimating a single regression (cannot be called) rule ols_model: input: script = config[&quot;src_analysis&quot;] + &quot;estimate_ols_model.R&quot;, data = data_mgt(config[&quot;out_data&quot;] + &quot;mrw_complete.csv&quot;), model = config[&quot;src_model_specs&quot;] + &quot;{iModel}.json&quot;, subset = config[&quot;src_data_specs&quot;] + &quot;{iSubset}.json&quot; output: model_est = config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, log: config[&quot;log&quot;] + &quot;analysis/{iModel}_ols_{iSubset}.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.model_est} \\ &gt;&amp; {log}&quot; Following the logic in 11.1, we then update the snakefile in root by: removing the rules we copied over adding a subworkflow called analysis ensuring if we use outputs created in analysis subworkflow anywhere else, we wrap them in analysis(output) Doing (1)-(3) in our main Snakefile, suggests we update the following: (list) Let’s do it: &lt;...&gt; # --- Sub Workflows --- # subworkflow data_mgt: workdir: config[&quot;ROOT&quot;] snakefile: config[&quot;src_data_mgt&quot;] + &quot;Snakefile&quot; subworkflow analysis: workdir: config[&quot;ROOT&quot;] snakefile: config[&quot;src_analysis&quot;] + &quot;Snakefile&quot; # --- Build Rules --- # ## all : builds all final outputs rule all: input: figs = expand(config[&quot;out_figures&quot;] + &quot;{iFigure}.pdf&quot;, iFigure = FIGURES), models = analysis(expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) ), tables = expand(config[&quot;out_tables&quot;] + &quot;{iTable}.tex&quot;, iTable = TABLES) ## augment_solow : construct a table of estimates for augmented solow model rule augment_solow: input: script = config[&quot;src_tables&quot;] + &quot;tab02_augment_solow.R&quot;, models = analysis(expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) ), params: filepath = config[&quot;out_analysis&quot;], model_expr = &quot;model_aug_solow*.rds&quot; output: table = config[&quot;out_tables&quot;] + &quot;tab02_augment_solow.tex&quot;, log: config[&quot;log&quot;] + &quot;tables/tab02_augment_solow.Rout&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table} \\ &gt;&amp; {log}&quot; ## textbook_solow : construct a table of regression estimates for textbook solow model rule textbook_solow: input: script = config[&quot;src_tables&quot;] + &quot;tab01_textbook_solow.R&quot;, models = analysis(expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) ), params: filepath = config[&quot;out_analysis&quot;], model_expr = &quot;model_solow*.rds&quot; output: table = config[&quot;out_tables&quot;] + &quot;tab01_textbook_solow.tex&quot; log: config[&quot;log&quot;] + &quot;tables/tab01_textbook_solow.Rout&quot; shell: &quot;Rscript {input.script} \\ --filepath {params.filepath} \\ --models {params.model_expr} \\ --out {output.table} \\ &gt;&amp; {log}&quot; ## make_figs : builds all figures rule make_figs: input: expand(config[&quot;out_figures&quot;] + &quot;{iFigure}.pdf&quot;, iFigure = FIGURES) ## figures : recipe for constructing a figure (cannot be called) rule figures: input: script = config[&quot;src_figures&quot;] + &quot;{iFigure}.R&quot;, data = data_mgt(config[&quot;out_data&quot;] + &quot;mrw_complete.csv&quot;), subset = config[&quot;src_data_specs&quot;] + &quot;subset_intermediate.json&quot; output: fig = config[&quot;out_figures&quot;] + &quot;{iFigure}.pdf&quot; log: config[&quot;log&quot;]+ &quot;figures/{iFigure}.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --subset {input.subset} \\ --out {output.fig} \\ &gt;&amp; {log}&quot; &lt;...&gt; Remark about dictionaries not having to be moved… Now if we again do a dry run with Snakemake let’s see what happens: Executing subworkflow analysis. Building DAG of jobs... Job counts: count jobs 24 ols_model 24 &lt;LIST OF ANALYSIS JOBS&gt; Executing subworkflow data_mgt. Building DAG of jobs... Job counts: count jobs 1 gen_regression_vars 1 rename_vars 2 &lt;LIST OF DATA-MGT JOBS&gt; Executing main workflow. Job counts: count jobs 1 all 1 augment_solow 3 figures 1 textbook_solow 6 &lt;LIST OF MAIN WORKFLOW JOBS&gt; Snakemake tells us nothing is wrong. However, if we look at the order of execution: (list) it wants to run analysis workflow before data-mgt. This should be a problem. If we try and run snakemake and execute the workflow: $ snakemake We do get the expected error: Building DAG of jobs... Executing subworkflow analysis. Building DAG of jobs... Using shell: /bin/bash Provided cores: 1 Rules claiming more threads will be scaled down. Job counts: count jobs 24 ols_model 24 Traceback (most recent call last): File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/__init__.py&quot;, line 537, in snakemake report=report) File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/workflow.py&quot;, line 653, in execute success = scheduler.schedule() File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/scheduler.py&quot;, line 275, in schedule run = self.job_selector(needrun) File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/scheduler.py&quot;, line 399, in job_selector c = list(map(self.job_reward, jobs)) # job rewards File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/scheduler.py&quot;, line 469, in job_reward input_size = job.inputsize File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/jobs.py&quot;, line 288, in inputsize self._inputsize = sum(f.size for f in self.input) File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/jobs.py&quot;, line 288, in &lt;genexpr&gt; self._inputsize = sum(f.size for f in self.input) File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/io.py&quot;, line 123, in wrapper return func(self, *args, **kwargs) File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/io.py&quot;, line 138, in wrapper return func(self, *args, **kwargs) File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/io.py&quot;, line 286, in size return self.size_local File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/io.py&quot;, line 291, in size_local self.check_broken_symlink() File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/io.py&quot;, line 296, in check_broken_symlink if not self.exists_local and lstat(self.file): File &quot;/home/lachlan/anaconda3/lib/python3.5/site-packages/snakemake/io.py&quot;, line 29, in lstat follow_symlinks=os.stat not in os.supports_follow_symlinks) FileNotFoundError: [Errno 2] No such file or directory: &#39;/home/lachlan/teaching/snakemake-econ-r-learner/out/data/mrw_complete.csv&#39; This says that out/data/mrw_complete.csv doesn’t exist, therefore the models cannot be estimated. What’s the solution? We need to specify that the analysis subworkflow itself has a subworkflow, data_mgt, that needs to run before analysis. So we add the subworkflow data_mgt to the Snakefile in src/analysis: # subworkflow - analysis # # @yourname # # --- Importing Configuration Files --- # configfile: &quot;config.yaml&quot; # --- Sub Workflows --- # subworkflow data_mgt: workdir: config[&quot;ROOT&quot;] snakefile: config[&quot;src_data_mgt&quot;] + &quot;Snakefile&quot; # --- Build Rules --- # ## estimate_models : estimates all regressions rule estimate_models: input: expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) ## ols_models : recipe for estimating a single regression (cannot be called) rule ols_model: input: script = config[&quot;src_analysis&quot;] + &quot;estimate_ols_model.R&quot;, data = data_mgt(config[&quot;out_data&quot;] + &quot;mrw_complete.csv&quot;), model = config[&quot;src_model_specs&quot;] + &quot;{iModel}.json&quot;, subset = config[&quot;src_data_specs&quot;] + &quot;{iSubset}.json&quot; output: model_est = config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, log: config[&quot;log&quot;] + &quot;analysis/{iModel}_ols_{iSubset}.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.model_est} \\ &gt;&amp; {log}&quot; And we try again: $ snakemake Building DAG of jobs... Executing subworkflow data_mgt. Building DAG of jobs... Using shell: /bin/bash Provided cores: 1 Rules claiming more threads will be scaled down. Job counts: count jobs 1 gen_regression_vars 1 rename_vars 2 [Mon Feb 11 23:12:42 2019] rule rename_vars: input: src/data-management/rename_variables.R, src/data/mrw.dta output: out/data/mrw_renamed.csv log: logs/data-mgt/rename_variables.Rout jobid: 1 [Mon Feb 11 23:12:42 2019] Finished job 1. 1 of 2 steps (50%) done [Mon Feb 11 23:12:42 2019] rule gen_regression_vars: input: src/data-management/gen_reg_vars.R, src/data-specs/param_solow.json, out/data/mrw_renamed.csv output: out/data/mrw_complete.csv log: logs/data-mgt/gen_reg_vars.Rout jobid: 0 [Mon Feb 11 23:12:43 2019] Finished job 0. 2 of 2 steps (100%) done Complete log: /home/lachlan/teaching/snakemake-econ-r-learner/.snakemake/log/2019-02-11T231242.348577.snakemake.log Executing subworkflow analysis. Building DAG of jobs... Executing subworkflow data_mgt. Error: Snakefile &quot;/home/lachlan/teaching/snakemake-econ-r-learner/src/analysis/src/data-management/Snakefile&quot; not present. What has happened this time? its about the path explain why this fix works: # subworkflow - analysis # # @yourname # # --- Importing Configuration Files --- # configfile: &quot;config.yaml&quot; # --- Sub Workflows --- # subworkflow data_mgt: workdir: config[&quot;sub2root&quot;] + config[&quot;ROOT&quot;] snakefile: config[&quot;sub2root&quot;]+ config[&quot;src_data_mgt&quot;] + &quot;Snakefile&quot; # --- Build Rules --- # ## estimate_models : estimates all regressions rule estimate_models: input: expand(config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, iModel = MODELS, iSubset = DATA_SUBSET) ## ols_models : recipe for estimating a single regression (cannot be called) rule ols_model: input: script = config[&quot;src_analysis&quot;] + &quot;estimate_ols_model.R&quot;, data = data_mgt(config[&quot;out_data&quot;] + &quot;mrw_complete.csv&quot;), model = config[&quot;src_model_specs&quot;] + &quot;{iModel}.json&quot;, subset = config[&quot;src_data_specs&quot;] + &quot;{iSubset}.json&quot; output: model_est = config[&quot;out_analysis&quot;] + &quot;{iModel}_ols_{iSubset}.rds&quot;, log: config[&quot;log&quot;] + &quot;analysis/{iModel}_ols_{iSubset}.Rout&quot; shell: &quot;Rscript {input.script} \\ --data {input.data} \\ --model {input.model} \\ --subset {input.subset} \\ --out {output.model_est} \\ &gt;&amp; {log}&quot; Now if we run snakemake again: $ snakemake Our build runs from start to finish. Exercise: More Subworkflows Make two new subworkflows figs and tables to contain the rules to construct all figures and tables respectively. The end result should be only the all rule in the “Main Build Rules” section of the Snakefile of the root directory. Be sure to carefully think about properly adding subworkflows, and subworkflow dependencies. To check your new subworkflow system builds, clean the output directory and be sure after entering snakemake into the terminal that all outputs are successfully created. Guided Exercise: help rules with subworkflows With the new subworkflow system in place, examine how the output from snakemake help looks like. To bring our help rule back into shape, replace the shell command in the help rule with: find . -type f -name &#39;Snakefile&#39; | tac | xargs sed -n &#39;s/^##//p&#39; \\ &gt; {output} Explain exactly what this command is doing. (Look for help in ‘usual’ places). Next, update the inputs of the help rule so that the dependencies are correct. "],["part-iii.html", "PART III", " PART III "],["reproducible-articles-with-rmd.html", "Chapter 16 Reproducible Articles with Rmd", " Chapter 16 Reproducible Articles with Rmd Content is TBD "],["reproducible-slides-with-rmd.html", "Chapter 17 Reproducible Slides with Rmd", " Chapter 17 Reproducible Slides with Rmd Content TBD "],["package-dependencies-with-packrat.html", "Chapter 18 Package Dependencies with Packrat", " Chapter 18 Package Dependencies with Packrat "],["containers.html", "Chapter 19 Containers", " Chapter 19 Containers "]]
